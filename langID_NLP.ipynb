{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of langID_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dagobert42/langID-NLP/blob/master/langID_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgCjPUC4Sk86"
      },
      "source": [
        "### 1. Dataset Preparation\n",
        "This subsection contains methods to produce uniformly distributed chunks of our data set. From these we can then obtain n-grams of different sizes. The Wikipedia Language Identification database contains txt-files of x_train and x_test for example sentences and accordingly ordered labels in y_train, y_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt1fyVMmyyc5"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import string, re, time, math\r\n",
        "import nltk\r\n",
        "from nltk import ngrams\r\n",
        "import collections\r\n",
        "from collections import defaultdict\r\n",
        "from collections import Counter\r\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFQqznQJjLdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516d30ea-9c94-404b-9887-7aa5a7ce3a16"
      },
      "source": [
        "!npx degit Dagobert42/langID-NLP/WiLI-2018_data -f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l[..................] / rollbackFailedOptional: verb npm-session 2352d61852ac73b\u001b[0m\u001b[K\r[#######...........] \\ extract:degit: verb lock using /root/.npm/_locks/staging\u001b[0m\u001b[K\r[#######...........] \\ extract:degit: verb lock using /root/.npm/_locks/staging\u001b[0m\u001b[K\r\r\u001b[K\u001b[?25hnpx: installed 1 in 0.715s\n",
            "\u001b[36m> destination directory is not empty. Using --force, continuing\u001b[39m\n",
            "\u001b[36m> cloned \u001b[1mDagobert42/langID-NLP\u001b[22m#\u001b[1mmaster\u001b[22m\u001b[39m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v0_QzWx9XRU"
      },
      "source": [
        "# read data\r\n",
        "# written for the WiLI-2018 data set: https://zenodo.org/record/841984\r\n",
        "X_train = open('x_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_train = open('y_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "X_test = open('x_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_test = open('y_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "labels = pd.read_csv('labels.csv', delimiter = ';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njIRm3SO9xiV"
      },
      "source": [
        "# preprocessing the data\r\n",
        "def preprocess(X,Y):\r\n",
        "\r\n",
        "    # convert language labels to language Name => 'en' -> 'English'\r\n",
        "    lab_dict = { labels.loc[i]['Label'] : labels.loc[i]['English'] for i in range(0, len(labels)) }\r\n",
        "    y_train = [ lab_dict[item] if item != 'nan' else 'Min Nan Chinese' for item in Y ]\r\n",
        "\r\n",
        "    # remove unnecessary characters from data\r\n",
        "    extras = '!\"$%&/{}[]()=?\\\\`´*+~#-_.:,;<>|1234567890°-\\''\r\n",
        "    rx = '[' + re.escape(''.join(extras)) + ']'\r\n",
        "    x_train = [] \r\n",
        "    to_remove = []\r\n",
        "    i = 0\r\n",
        "    for example in X:\r\n",
        "        processed = re.sub(' +', ' ', re.sub(rx, '', example))\r\n",
        "        # some examples only contain spaces after preprocessing\r\n",
        "        # this is a check for those examples\r\n",
        "        if len(\"\".join(processed.split())):\r\n",
        "            x_train.append(processed)\r\n",
        "        else:\r\n",
        "            y_train.pop(i)\r\n",
        "        i+=1\r\n",
        "\r\n",
        "    return x_train,y_train\r\n",
        "\r\n",
        "# Output example:\r\n",
        "# x_train = [ex1, ex2, ex3, ...]\r\n",
        "# y_train = [lang_of_ex1, ...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh1WIZ_bAkZA"
      },
      "source": [
        "# sort data by language\r\n",
        "def data_by_lang(X, Y):\r\n",
        "    lang_corpora = defaultdict(list)\r\n",
        "    lang_idx = defaultdict(list)\r\n",
        "    for i in range(len(X)):\r\n",
        "        lang_corpora[Y[i]].append(X[i])\r\n",
        "        lang_idx[Y[i]].append(i)\r\n",
        "\r\n",
        "    return lang_corpora, lang_idx\r\n",
        "    \r\n",
        "# Output example:\r\n",
        "# lang_corpora = { 'Lang1' : [ex1, ex2, ...], 'Lang2' : [ex1, ex2, , ...], ...}\r\n",
        "# land_idx = { 'Lang1' : [23, 41, ...], 'Lang2' : [1, 19, ...], ...}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNBMCn3bA078"
      },
      "source": [
        "# extract uniformly distributed list of examples from our data set\r\n",
        "# takes an optional argument to constrain the list of languages\r\n",
        "def get_data_chunk(X, Y, n_samples, lang_keys=[]):\r\n",
        "    _, lang_idx = data_by_lang(X, Y)\r\n",
        "    x_train = []\r\n",
        "    y_train = []\r\n",
        "    \r\n",
        "    langs = set()\r\n",
        "    if lang_keys: \r\n",
        "        langs = set(lang_keys)\r\n",
        "    else:\r\n",
        "        langs = set(Y)\r\n",
        "\r\n",
        "    for lang in langs:\r\n",
        "        indices = lang_idx[lang]\r\n",
        "        if(len(indices) < n_instances):\r\n",
        "          n_instances = len(indices)\r\n",
        "        for index in range(n_samples):\r\n",
        "            x_train.append(X[indices[index]])\r\n",
        "            y_train.append(Y[indices[index]])\r\n",
        "\r\n",
        "    return x_train, y_train\r\n",
        "\r\n",
        "# Output example:\r\n",
        "# x_train [ lang1_ex1, lang1_ex2, ..., lang2_ex, lang2_ex, ...]\r\n",
        "# y_train [ lang1, lang1, ...lang2, lang2, ...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_gdP0c8ZGtJ"
      },
      "source": [
        "# creating n-grams for each data entry\r\n",
        "# optional arguments:\r\n",
        "#    lang_keys - constrains the languages to use\r\n",
        "#    stepsize  - specifies the amount of characters\r\n",
        "#                to jump until the next n-gram\r\n",
        "# returns a list of n-grams\r\n",
        "def make_n_grams(X, Y, n, lang_keys=[], stepsize=1):\r\n",
        "    assert stepsize >= 1\r\n",
        "    x_to_grams = []\r\n",
        "\r\n",
        "    langs = set()\r\n",
        "    if lang_keys: \r\n",
        "        langs = set(lang_keys)\r\n",
        "    else:\r\n",
        "        langs = set(Y)\r\n",
        "\r\n",
        "    for i in range(len(X)):\r\n",
        "        if Y[i] in langs:\r\n",
        "            sent = X[i]\r\n",
        "            x_to_grams.append([sent[j:j+n] for j in range(0, len(sent) - n+1, stepsize)])\r\n",
        "\r\n",
        "    return x_to_grams\r\n",
        "\r\n",
        "# Output example:\r\n",
        "# x_to_grams = [[ngram_in_ex1], [ngram_in_ex2], ...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT5Bp8t6ZJyz"
      },
      "source": [
        "# counting and sorting n-grams for each language\r\n",
        "# returns a sorted dict of lang : {n-gram : count}\r\n",
        "def sort_by_tf(X, Y):\r\n",
        "    # calculating term frequency of n-grams per language\r\n",
        "    tf_per_lang = defaultdict(list)\r\n",
        "    langs = set(Y)\r\n",
        "    data, _ = data_by_lang(X, Y)\r\n",
        "    for lang,gram_list in data.items():\r\n",
        "      data[lang] = [ gram for grams in gram_list for gram in grams] # Comvert list of lists to a single list\r\n",
        "    for lang in langs:\r\n",
        "        tf_per_lang[lang] = dict(\r\n",
        "            zip(list(Counter(data[lang]).keys()),\r\n",
        "                 list(Counter(data[lang]).values())))\r\n",
        "\r\n",
        "    # sort by term frequency\r\n",
        "    sorted_tf_per_lang = defaultdict(list)\r\n",
        "    for lang in langs:\r\n",
        "        sorted_tf_per_lang[lang] = { word : value for word, value in sorted(tf_per_lang[lang].items(), key=lambda item:item[1], reverse=True) }\r\n",
        "    \r\n",
        "    return sorted_tf_per_lang\r\n",
        "# Output example:\r\n",
        "# sorted_tf_per_lang = { lang1 : {n_gram1 : count}, ...}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3cYY3GVpMy"
      },
      "source": [
        "### Understanding Data\n",
        "n the following we review some examples to get an understanding of our data.\n",
        "Particularly interesting are languages with a degree of similarity, as these are a greater challenge for classification. Here we print examples of n-grams from languages that use the Latin alphabet.\n",
        "Starting with trigrams, and taking the 20 most common from each language, we see that many languages have a few in common. This is above all the case with languages which are closely related. Here we have examples from the Latin family, the Germanic family, and single examples from their families (Estonian, Turkish). At the trigram level, we even see plenty of intersection between languages which aren't related at all (French, Estonian), or only distantly (German, Romanian)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2ZhYxV2Fami"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUM_YDQ4dqkD"
      },
      "source": [
        "def make_barplots(n_gram_size, lang_key, commoncollect, base_counts, compare_counts, otherlangs, concise=True):\n",
        "    width = 0.35  # the width of the bars\n",
        "    if len(otherlangs) > 6 and concise==False:\n",
        "        nrows = 3\n",
        "        ncols = 3\n",
        "        figsize=(22, 10)\n",
        "    elif len(otherlangs) > 4 and concise==False:\n",
        "        nrows = 2\n",
        "        ncols = 3\n",
        "        figsize=(22, 10)\n",
        "    elif len(otherlangs) > 1 and concise==False:\n",
        "        nrows = 1\n",
        "        ncols = len(otherlangs)\n",
        "        if len(otherlangs) == 4:\n",
        "            figsize=(26, 5)\n",
        "        else:\n",
        "            figsize=(18, 4)\n",
        "    elif len(otherlangs) == 1 or concise==True:\n",
        "        try:\n",
        "            fig, ax = plt.subplots(figsize=(7, 4))\n",
        "            fig.suptitle('Frequency of %s-grams in %s compared to %s:' % (n_gram_size, lang_key, otherlangs[0]),  fontsize=16)\n",
        "            x = np.arange(len(commoncollect[0]))  # the label locations   \n",
        "            rects1 = ax.bar(x - width/2, base_counts[0], width, color = 'r', label=lang_key)\n",
        "            rects2 = ax.bar(x + width/2, compare_counts[0], width, color = 'g', label=otherlangs[0])\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels(commoncollect[0], fontsize=14)\n",
        "            ax.legend(fontsize=12)\n",
        "            plt.show()\n",
        "        except IndexError:\n",
        "            print('there was an IndexError')\n",
        "            return\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, sharey = 'row', figsize=figsize)\n",
        "    fig.suptitle('Frequency of %s-grams in %s compared to other languages:' % (n_gram_size, lang_key),  fontsize=20)\n",
        "    for ax, common_ngrams, counts_langkey, counts_otherlang, otherlang in zip(axes.flatten(), commoncollect, base_counts, compare_counts, otherlangs):\n",
        "        x = np.arange(len(common_ngrams)) \n",
        "        rects1 = ax.bar(x - width/2, counts_langkey, width, color = 'r', label=lang_key)\n",
        "        rects2 = ax.bar(x + width/2, counts_otherlang, width, color = 'g', label=otherlang)\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(common_ngrams, fontsize=14)\n",
        "        ax.legend(fontsize=12)\n",
        "        \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGcqY6z7Fsyf"
      },
      "source": [
        "As we increase the size of our n-grams, the number of examples declines dramatically, as does the intersection between the commonest n-grams in various languages. Even at 4-grams, we see that the Germanic and Latin families are delineated from each other. Higher than that, and only the Latin languages still have some n-grams in common with each other, at least among the top 20 most frequent. This suggests that longer n-grams can more quickly lead to accurate identification of a language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC_uPl7fMbOf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d3c88cd6-dfc4-4f12-ee31-4e7c7aa1f4d3"
      },
      "source": [
        "'''\n",
        "x_train, y_train = preprocess(X_train[:-1],Y_train[:-1])\n",
        "\n",
        "latin_languages = ['German', 'English', 'French', 'Spanish', 'Italian', 'Portuguese', \n",
        "                    'Estonian', 'Turkish', 'Romanian', 'Swedish', 'Latin', 'Dutch']\n",
        "                    \n",
        "# produces charts of counts of common n-grams in different languages, and tables suggesting similar languages based on these\n",
        "\n",
        "for n_gram_size in range(4,7):\n",
        "    m_samples = 20\n",
        "\n",
        "    ng_related = {}\n",
        "    \n",
        "    x_train_grams = make_n_grams(x_train, y_train, n_gram_size, stepsize = n_gram_size)\n",
        "    sorted_tf_per_lang = sort_by_tf(x_train_grams, y_train)\n",
        "\n",
        "    for lang_key in latin_languages:\n",
        "\n",
        "        ng_related[lang_key] = []\n",
        "        latin_languages.remove(lang_key)\n",
        "        latin_langs = latin_languages\n",
        "        top_m = list(sorted_tf_per_lang[lang_key].keys())[:m_samples]\n",
        "\n",
        "        commoncollect, base_counts, compare_counts, otherlangs = [], [], [], []\n",
        "\n",
        "        for otherlang in latin_langs:\n",
        "            top_m_x = list(sorted_tf_per_lang[otherlang].keys())[:m_samples]\n",
        "            common_ngrams = list(set(top_m).intersection(top_m_x))\n",
        "            \n",
        "            if len(common_ngrams) > 2:\n",
        "                \n",
        "                ng_related[lang_key].append(otherlang)\n",
        "                \n",
        "                # get counts of the entries in common_ngrams for each language.\n",
        "                # These are stored as the values corresponding to the n-gram keys in the dictionary\n",
        "                counts_langkey = []\n",
        "                counts_otherlang = []\n",
        "                for i in common_ngrams:\n",
        "                    counts_langkey.append(sorted_tf_per_lang[lang_key][i])\n",
        "                    counts_otherlang.append(sorted_tf_per_lang[otherlang][i])\n",
        "\n",
        "                common_ngrams = [k.replace(' ', '_') for k in common_ngrams]\n",
        "                \n",
        "                #print(lang_key, \"and\", otherlang, \"have the following frequent\", n_gram_size,\"-grams in common:\",common_ngrams)\n",
        "                #print('\\n ')\n",
        "\n",
        "                commoncollect.append(common_ngrams)\n",
        "                base_counts.append(counts_langkey)\n",
        "                compare_counts.append(counts_otherlang)\n",
        "                otherlangs.append(otherlang)\n",
        "\n",
        "        if commoncollect:\n",
        "        # this sorting operation is to position the largest n-gram intersections at the top of the lists.\n",
        "            commoncollect, base_counts, compare_counts, otherlangs = zip(*sorted(zip(commoncollect, base_counts, compare_counts, otherlangs), key=lambda x: len(x[0]), reverse=True))\n",
        " \n",
        "            # set parameter \"concise\" to False if you want an extensive set of bar charts in the output\n",
        "            make_barplots(n_gram_size, lang_key, commoncollect, \n",
        "                          base_counts, compare_counts, otherlangs, concise=True)\n",
        "        \n",
        "        latin_languages = ['German', 'English', 'French', 'Spanish', 'Italian', 'Portuguese', \n",
        "                           'Estonian', 'Turkish', 'Romanian', 'Swedish', 'Latin', 'Dutch']                    \n",
        "    \n",
        "    print('similar languages based on ', n_gram_size, '- grams:')\n",
        "    for key, val in ng_related.items():\n",
        "        print(key, ':', val)\n",
        "    \n",
        "    print('\\n ')\n",
        "\n",
        "#time.sleep(120)\n",
        "clear_output()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nx_train, y_train = preprocess(X_train[:-1],Y_train[:-1])\\n\\nlatin_languages = [\\'German\\', \\'English\\', \\'French\\', \\'Spanish\\', \\'Italian\\', \\'Portuguese\\', \\n                    \\'Estonian\\', \\'Turkish\\', \\'Romanian\\', \\'Swedish\\', \\'Latin\\', \\'Dutch\\']\\n                    \\n# produces charts of counts of common n-grams in different languages, and tables suggesting similar languages based on these\\n\\nfor n_gram_size in range(4,7):\\n    m_samples = 20\\n\\n    ng_related = {}\\n    \\n    x_train_grams = make_n_grams(x_train, y_train, n_gram_size, stepsize = n_gram_size)\\n    sorted_tf_per_lang = sort_by_tf(x_train_grams, y_train)\\n\\n    for lang_key in latin_languages:\\n\\n        ng_related[lang_key] = []\\n        latin_languages.remove(lang_key)\\n        latin_langs = latin_languages\\n        top_m = list(sorted_tf_per_lang[lang_key].keys())[:m_samples]\\n\\n        commoncollect, base_counts, compare_counts, otherlangs = [], [], [], []\\n\\n        for otherlang in latin_langs:\\n            top_m_x = list(sorted_tf_per_lang[otherlang].keys())[:m_samples]\\n            common_ngrams = list(set(top_m).intersection(top_m_x))\\n            \\n            if len(common_ngrams) > 2:\\n                \\n                ng_related[lang_key].append(otherlang)\\n                \\n                # get counts of the entries in common_ngrams for each language.\\n                # These are stored as the values corresponding to the n-gram keys in the dictionary\\n                counts_langkey = []\\n                counts_otherlang = []\\n                for i in common_ngrams:\\n                    counts_langkey.append(sorted_tf_per_lang[lang_key][i])\\n                    counts_otherlang.append(sorted_tf_per_lang[otherlang][i])\\n\\n                common_ngrams = [k.replace(\\' \\', \\'_\\') for k in common_ngrams]\\n                \\n                #print(lang_key, \"and\", otherlang, \"have the following frequent\", n_gram_size,\"-grams in common:\",common_ngrams)\\n                #print(\\'\\n \\')\\n\\n                commoncollect.append(common_ngrams)\\n                base_counts.append(counts_langkey)\\n                compare_counts.append(counts_otherlang)\\n                otherlangs.append(otherlang)\\n\\n        if commoncollect:\\n        # this sorting operation is to position the largest n-gram intersections at the top of the lists.\\n            commoncollect, base_counts, compare_counts, otherlangs = zip(*sorted(zip(commoncollect, base_counts, compare_counts, otherlangs), key=lambda x: len(x[0]), reverse=True))\\n \\n            # set parameter \"concise\" to False if you want an extensive set of bar charts in the output\\n            make_barplots(n_gram_size, lang_key, commoncollect, \\n                          base_counts, compare_counts, otherlangs, concise=True)\\n        \\n        latin_languages = [\\'German\\', \\'English\\', \\'French\\', \\'Spanish\\', \\'Italian\\', \\'Portuguese\\', \\n                           \\'Estonian\\', \\'Turkish\\', \\'Romanian\\', \\'Swedish\\', \\'Latin\\', \\'Dutch\\']                    \\n    \\n    print(\\'similar languages based on \\', n_gram_size, \\'- grams:\\')\\n    for key, val in ng_related.items():\\n        print(key, \\':\\', val)\\n    \\n    print(\\'\\n \\')\\n\\n#time.sleep(120)\\nclear_output()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3vFh28hYXfA"
      },
      "source": [
        "### Naive Bayes Classifier\n",
        "To obtain a baseline for the language identification task we employ a Gaussian Naive Bayes classifier.\n",
        "\n",
        "!!!!! add some observations and F-1 scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlUCSGN9Wg3l"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import time,math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07I5K2SxKXWF"
      },
      "source": [
        "def time_since(since):\r\n",
        "    s = time.time() - since\r\n",
        "    m = math.floor(s / 60)\r\n",
        "    s -= m * 60\r\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stiyBC09ePY0"
      },
      "source": [
        "# extracts lists of top n frequent n-grams from data\r\n",
        "def get_top_n_features(X_grams, Y, n_features):\r\n",
        "  '''\r\n",
        "  X : [[ngram_in_ex1], [ngram_in_ex2], ...]\r\n",
        "  Y : ['lang1', 'lang1', ..., 'lang2', ...]\r\n",
        "  n_features : number of ngram to pick from each language\r\n",
        "  '''\r\n",
        "  sorted_freq_per_lang = sort_by_tf(X_grams, Y)\r\n",
        "  features = []\r\n",
        "  for lang, grams_dict in sorted_freq_per_lang.items():\r\n",
        "      i = 0\r\n",
        "      for gram, count in grams_dict.items():\r\n",
        "          if i <= n_features:\r\n",
        "              features.append(gram)\r\n",
        "          else:\r\n",
        "              break\r\n",
        "          i += 1\r\n",
        "      \r\n",
        "  return list(set(features))\r\n",
        "# Output example:\r\n",
        "# features : ['and', 'he ', 'öä ', ...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUr77bPVeVQm"
      },
      "source": [
        "# convert data to feature matrix\r\n",
        "def create_feature_matrix(X, features):\r\n",
        "  '''\r\n",
        "  X : [[ngram_in_ex1], [ngram_in_ex2], ...]\r\n",
        "  features : ['and', 'he ', 'öä ', ...] top ngrams from corpus\r\n",
        "\r\n",
        "  '''\r\n",
        "  mat = np.zeros((len(X),len(features)))\r\n",
        "  i = 0\r\n",
        "  for gram_list in X:\r\n",
        "      gram_count = []\r\n",
        "      for gram in features:          \r\n",
        "          if gram in gram_list:\r\n",
        "              gram_count.append(gram_list.count(gram)+1)\r\n",
        "          else:\r\n",
        "              gram_count.append(1)\r\n",
        "      mat[i] = gram_count\r\n",
        "      i+=1\r\n",
        "\r\n",
        "  return mat\r\n",
        "  # mat : array([[4,1,2,1,...],[1,1,1,2,3,1,...],...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaeJeeEvRtDd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e00fe1d9-e93e-40e2-ffaf-6ff2688eb7f6"
      },
      "source": [
        "n_instances = 500 # instance per language, must be less than 500\r\n",
        "n_gram_size = 4\r\n",
        "n_features = 20 # features per language\r\n",
        "n_lang = 11\r\n",
        "start = time.time()\r\n",
        "print(\"Starting preprocessing at {} ..\".format(time_since(start)))\r\n",
        "\r\n",
        "x_train, y_train = preprocess(X_train[:-1], Y_train[:-1])\r\n",
        "\r\n",
        "print(\"Preprocessing Done at {}.\".format(time_since(start)))\r\n",
        "\r\n",
        "avail_lang = list(set(y_train))\r\n",
        "rand_idx = random.sample(range(0,235),n_lang)\r\n",
        "lang_group = np.array(avail_lang)[rand_idx] # Selecting n_lang random language for training\r\n",
        "\r\n",
        "# reduce languages to get smaller data subset\r\n",
        "x_train, y_train = get_data_chunk(x_train, y_train, n_instances,dsl_groups_ABC)\r\n",
        "\r\n",
        "print(\"Making ngrams at {} ..\".format(time_since(start)))\r\n",
        "x_train_grams = make_n_grams(x_train, y_train, n_gram_size)\r\n",
        "\r\n",
        "print('Extracting features at {} ...'.format(time_since(start)))\r\n",
        "# create features for dataset\r\n",
        "features = get_top_n_features(x_train_grams, y_train, n_features)\r\n",
        "\r\n",
        "print('Creating feature matrix at {}  ....'.format(time_since(start)))\r\n",
        "# convert dataset into feature matrix\r\n",
        "feature_matrix = create_feature_matrix(x_train_grams, features)\r\n",
        "print('Data Preperation completed after {}'.format(time_since(start)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nn_samples = 100 # instance per language\\nn_gram_size = 5\\nn_features = 10 # features per language\\nstart = time.time()\\nprint(\"Starting preprocessing at {} ..\".format(time_since(start)))\\n\\nx_train, y_train = preprocess(X_train[:-1], Y_train[:-1])\\n\\nprint(\"Preprocessing done at {}.\".format(time_since(start)))\\n\\n# reduce languages to get smaller data subset\\nx_train, y_train = get_data_chunk(x_train, y_train, n_samples)\\n\\nprint(\\'Making\\', str(n_gram_size) + \\'-grams at {} ..\\'.format(time_since(start)))\\nx_train_grams = make_n_grams(x_train, y_train, n_gram_size)\\n\\nprint(\\'Extracting\\', n_features, \\'features at {} ...\\'.format(time_since(start)))\\n# create features for dataset\\nfeatures = get_top_n_features(x_train_grams, y_train, n_features)\\n\\nprint(\\'Creating feature matrix at {}  ....\\'.format(time_since(start)))\\n# convert dataset into feature matrix\\nfeature_matrix = create_feature_matrix(x_train_grams, features)\\nprint(\\'Data Preperation completed after {}\\'.format(time_since(start)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsIdn1iIRu2g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "cce51124-15ff-4e31-fb03-572f611c3408"
      },
      "source": [
        "'''\r\n",
        "# Gaussian Naive Bayes Model Training\r\n",
        "encoder = LabelEncoder()\r\n",
        "X = feature_matrix\r\n",
        "Y = encoder.fit_transform(y_train)\r\n",
        "model = GaussianNB()\r\n",
        "model.fit(X,Y)\r\n",
        "'''\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Gaussian Naive Bayes Model Training\\nencoder = LabelEncoder()\\nX = feature_matrix\\nY = encoder.fit_transform(y_train)\\nmodel = GaussianNB()\\nmodel.fit(X,Y)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjTptZN9Rxxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "outputId": "d46f5eb7-219e-444a-c4cf-6ec92a62465a"
      },
      "source": [
        "# model testing\r\n",
        "start = time.time()\r\n",
        "print('Test Data preperation starting ...')\r\n",
        "x,y = preprocess(X_test[:-1], Y_test[:-1])\r\n",
        "\r\n",
        "n_instances = 500\r\n",
        "\r\n",
        "x,y = get_data_chunk(x,y,n_instances, dsl_groups_ABC)\r\n",
        "\r\n",
        "print(\"Making ngrams at {} ..\".format(time_since(start)))\r\n",
        "x_test_grams = make_n_grams(x, y, n_gram_size)\r\n",
        "\r\n",
        "print('Creating feature matrix at {}  ....'.format(time_since(start)))\r\n",
        "x = create_feature_matrix(x_test_grams, features)\r\n",
        "print('Test Data Preperation completed after {}'.format(time_since(start)))\r\n",
        "\r\n",
        "y = encoder.fit_transform(y)\r\n",
        "y_pred = model.predict(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# model testing\\nstart = time.time()\\nprint(\\'Test Data preperation starting ...\\')\\nx,y = preprocess(X_test[:20000], Y_test[:20000])\\n\\nprint(\"Making ngrams at {} ..\".format(time_since(start)))\\nx_test_grams = make_n_grams(x, y, n_gram_size)\\n\\nprint(\\'Creating feature matrix at {}  ....\\'.format(time_since(start)))\\nx = create_feature_matrix(x_test_grams, features)\\n\\nprint(\\'Test Data Preperation completed after {}\\'.format(time_since(start)))\\n\\ny = encoder.fit_transform(y)\\ny_pred = model.predict(x)\\nconf_matrix = confusion_matrix(y_pred=y_pred, y_true=y)\\nacc = round(accuracy_score(y_pred=y_pred, y_true=y), 4) * 100\\nprint(f\"Accuracy is {acc}%\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y01CODnMLh1-"
      },
      "source": [
        "# Model Evaluation\r\n",
        "\r\n",
        "acc = round(accuracy_score(y_pred=y_pred, y_true=y), 4) * 100\r\n",
        "print(f\"Accuracy is {acc}%\")\r\n",
        "\r\n",
        "f1score = f1_score(y,y_pred,average='weighted')\r\n",
        "print(f\"f1-score is {f1score}\")\r\n",
        "\r\n",
        "y1 = encoder.inverse_transform(y)\r\n",
        "y2 = encoder.inverse_transform(y_pred)\r\n",
        "conf_matrix = confusion_matrix(y_pred=y2, y_true=y1)\r\n",
        "df_cm = pd.DataFrame(conf_matrix/conf_matrix.astype(np.float).sum(axis=1), columns=np.unique(y1), index = np.unique(y1))\r\n",
        "df_cm.index.name = 'Actual'\r\n",
        "df_cm.columns.name = 'Predicted'\r\n",
        "plt.figure(figsize = (10,10))\r\n",
        "sns.set(font_scale=1.4)#for label size\r\n",
        "sns.heatmap(df_cm,annot=True,fmt=\".2f\",linewidths=1.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC1mI32qeqY8"
      },
      "source": [
        "### 4. Recurrent Neural Network\n",
        "\n",
        "In this section we introduce a Recurrent Neural Network (RNN) model for language identification task. The model is of a many-to-one type, meaning that we provide a sequence of data units (n-grams in our case) to multiple RNN cells which produces an output at the last cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3fZ8Vrmd5hi"
      },
      "source": [
        "import torch\r\n",
        "from torch.autograd import Variable \r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E74Oh7drBg4-"
      },
      "source": [
        "# helper to print progress bars of this length\n",
        "bar_len = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmHu-KycMevY"
      },
      "source": [
        "**CHANGE TO GPU MODE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFTaWPSMdqO"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQdjQUcgB3Ad"
      },
      "source": [
        "There are three layers in this model. First is an Embedding Layer which converts input data into embeddings, second is a Gated Recurrent Unit and finally a Linear Layer to provide the probability distribution over different languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPUswhM1qH8X"
      },
      "source": [
        "class RNN(nn.Module):\r\n",
        "    def __init__(self, input_size, hid_size, output_size, layers, embedding):\r\n",
        "\r\n",
        "        super(RNN, self).__init__()\r\n",
        "        self.hidden_dim = hid_size\r\n",
        "        self.layers = layers\r\n",
        "        self.embedding_size = embedding \r\n",
        "        self.dropout = nn.Dropout(0.4)\r\n",
        "        self.input_size = input_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.embeddings = nn.Embedding(self.input_size,self.embedding_size)\r\n",
        "        self.rnn = nn.GRU(input_size=self.embedding_size,hidden_size=self.hidden_dim,num_layers = self.layers)\r\n",
        "        self.linear = nn.Linear(self.hidden_dim,self.output_size)\r\n",
        "\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        # x : B x S where B is batch size and S is sequence size\r\n",
        "        # Sequence size is length of one ngram vector encoding\r\n",
        "        batch_size = x.size(0) \r\n",
        "        x = x.t() \r\n",
        "        embedded = self.embeddings(x) # S x B x I , here I is input_size/vocab size\r\n",
        "        hidden = self._init_hidden(batch_size)\r\n",
        "        output,hidden = self.rnn(embedded,hidden)\r\n",
        "        output = self.dropout(output) \r\n",
        "        fc_output = self.linear(output[-1]) # B x L , L is number of classes/languages\r\n",
        "        return fc_output\r\n",
        "\r\n",
        "    def _init_hidden(self,batch_size):\r\n",
        "        hidden_state = torch.zeros(self.layers,batch_size, self.hidden_dim, device=device)\r\n",
        "        return hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLmwqw5wA_Jb"
      },
      "source": [
        "First we extract a number of samples from each language of the given corpus. It is also possible to constrain the data to a subset of languages. Each instance in this reduced dataset is converted into n-grams of a fixed length. Our vocabulary is created by choosing a number of top n-grams by term-frequency and then mapping them to an index which we can use to create encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlTIy_zRkxNE"
      },
      "source": [
        "def prepare_data(X, Y, n_samples, n_gram_size, n_features, lang_keys=[]):\r\n",
        "    '''\r\n",
        "    Data is prepared in 3 steps:\r\n",
        "    1. ngram is created from given dataset i.e each example is converted into a list of ngram\r\n",
        "    2. These ngrams are used to create vocabulary\r\n",
        "    3. Finally dataset is converted into encodings to feed into RNN\r\n",
        "    '''\r\n",
        "\r\n",
        "    # reduce languages to get smaller data subset\r\n",
        "    print('Preparing', n_samples, 'samples per language')\r\n",
        "    x_train, y_train = get_data_chunk(X, Y, n_samples, lang_keys)\r\n",
        "\r\n",
        "    print('Making', str(n_gram_size) + '-grams at {} ..'.format(time_since(start)))\r\n",
        "    x_train_grams = make_n_grams(x_train, y_train, n_gram_size, lang_keys)\r\n",
        "\r\n",
        "    # create features for dataset\r\n",
        "    print('Extracting', n_features, 'features at {} ...'.format(time_since(start)))\r\n",
        "    features = get_top_n_features(x_train_grams, y_train, n_features)\r\n",
        "    vocabulary = {word: i for i, word in enumerate(features)}\r\n",
        "\r\n",
        "    inp, target, _ = create_encodings(x_train_grams, y_train, vocabulary)\r\n",
        "    return vocabulary, inp, target, "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxPaHMhSDzbX"
      },
      "source": [
        "N-grams in each example are converted into encodings using our indeces. For each sample, if an n-gram is present in our vocabulary, its index is used as an encoding. This results in a vector of integers for each example. However, these vectors can be of different lengths, which might create problems when we train a model with a batch of input data. So in order to make vectors of equal length we pad it with zeros at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRlIup7rK5RY"
      },
      "source": [
        "def create_encodings(X_grams, Y, vocabulary):\r\n",
        "    '''\r\n",
        "    X_grams : [[ngram_in_ex1],[ngram_in_ex2],....]\r\n",
        "    Y : ['lang1','lang1',...,'lang2',....]\r\n",
        "    vocabulary : {' of':1,'apf':2,....}\r\n",
        "    '''\r\n",
        "    x_grams = []\r\n",
        "    y_grams = []\r\n",
        "    gram_len = []\r\n",
        "    iter = len(X_grams)/10\r\n",
        "    print('Creating Encoding for X')\r\n",
        "    for j in range(len(X_grams)):\r\n",
        "        gramlist = X_grams[j] # list of ngrams in example j\r\n",
        "        gramlist = list(dict.fromkeys(sorted(gramlist,key=gramlist.count,reverse=True)))\r\n",
        "        grams = [vocabulary[w] for w in gramlist if w in list(vocabulary.keys()) ] \r\n",
        "        \r\n",
        "        if (len(grams) >= 1): # resulting grams list must not be empty\r\n",
        "            x_grams.append(grams) # Add encodings to x_grams\r\n",
        "            gram_len.append(len(grams)) # Add corresponding data\r\n",
        "            y_grams.append(Y[j])\r\n",
        "\r\n",
        "        if (j % iter == 0):\r\n",
        "            print('\\r{}% data prepared at time {}'.format((j/len(X_grams))*100, time_since(start)), end=' ')\r\n",
        "    \r\n",
        "    print('\\r{}% data prepared at time {}'.format(100, time_since(start)), end=' ')\r\n",
        "    gram_len = torch.LongTensor(gram_len)\r\n",
        "    inp = padding(x_grams,gram_len)\r\n",
        "\r\n",
        "    print('\\nCreating Encoding for Y')\r\n",
        "    label = list(set(y_grams))\r\n",
        "    labels_to_idx = { lang:i  for i,lang in enumerate(label)}\r\n",
        "    y_label = torch.zeros(len(y_grams),device=device).long()\r\n",
        "    for i in range(len(y_label)):\r\n",
        "        y_label[i] = labels_to_idx[y_grams[i]]\r\n",
        "    target = Variable(torch.cuda.LongTensor(y_label))\r\n",
        "    return inp, target, labels_to_idx\r\n",
        "\r\n",
        "  # inp : tensor([[12, 342, ..., 0],[56, 2311, ...], ...])\r\n",
        "  # target : tensor([41, 127, 234, 16, ...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ennmQVVXqfI4"
      },
      "source": [
        "def padding(vector_inps, lengths):\r\n",
        "    '''\r\n",
        "    This function takes variable lengths vectors and convert them into equal length by padding 0.\r\n",
        "    Input : \"vector_inps\" list of vectors containing indices of ngrams\r\n",
        "            \"lengths \" length of each vector\r\n",
        "    Output : Tensor containing vectors of equal length after padding\r\n",
        "             This length is equal to maximum number(M) in list of \"lengths\"\r\n",
        "    '''\r\n",
        "    inp_tensor = torch.zeros((len(vector_inps),lengths.max()), device= device).long()\r\n",
        "    for idx, (seq, seq_len) in enumerate(zip(vector_inps,lengths)):\r\n",
        "        inp_tensor[idx, :seq_len] = torch.LongTensor(seq)\r\n",
        "    return inp_tensor\r\n",
        "\r\n",
        "    # inp_tensor : tensor([[12, 342, ..., 0], [56, 2311, ...], ...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snUkxMh3vGjJ"
      },
      "source": [
        "def start_training(X, Y, input_size, hidden_size=64, n_layers=1, embedding_size=64, lr=0.001, batch=64, n_epochs=1000):\r\n",
        "    output_size = len(set(Y.cpu().numpy())) # number of languages\r\n",
        "    model = RNN(input_size, hidden_size, output_size, n_layers, embedding_size).to(device)\r\n",
        "    decoder_optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    print_every = n_epochs/10\r\n",
        "    all_losses = []\r\n",
        "    f1_scores = []\r\n",
        "    print('Training:')\r\n",
        "    for epoch in range(1, n_epochs+1):\r\n",
        "        loss, score = train(model, criterion, decoder_optimizer, X, target, batch)\r\n",
        "        all_losses.append(loss)\r\n",
        "        f1_scores.append(score)\r\n",
        "        if(epoch % print_every == 0):\r\n",
        "            i = math.ceil(epoch/n_epochs * bar_len)\r\n",
        "            print('\\r', '#' * i, ' ' * (bar_len-i), epoch/n_epochs * 100, '%',\r\n",
        "                  '[{} epoch: {} loss: {:.4f}]'.format(time_since(start), epoch, loss), end=' ')\r\n",
        "    return model, all_losses, f1_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU5wFzymqguQ"
      },
      "source": [
        "def train(decoder, criterion, decoder_optimizer, inp, target, batch = 100):\r\n",
        "    '''\r\n",
        "    decoder : Model\r\n",
        "    inp : tensor([[12, 342, ..., 0],[56, 2311, ...], ...])\r\n",
        "    target : tensor([41, 127, 234, 16, ...])\r\n",
        "    '''\r\n",
        "    decoder.zero_grad()\r\n",
        "    loss = 0\r\n",
        "    score = 0\r\n",
        "    for i in range(1,int(len(inp)/batch)):\r\n",
        "        # input to model should be in form B x S where B is batch size and S is sequence size\r\n",
        "        output = decoder(inp[(i-1) * batch: (i) * batch].view(batch, -1))\r\n",
        "        loss += criterion(output, target[(i-1)*batch:(i)*batch])\r\n",
        "        out = output.argmax(dim=1)\r\n",
        "        score += f1_score(target[(i-1)*batch:(i)*batch].cpu().numpy(),out.cpu().numpy(),average='weighted')\r\n",
        "    loss.backward()\r\n",
        "    decoder_optimizer.step()\r\n",
        "    score = score/int(len(inp)/batch)\r\n",
        "\r\n",
        "    return loss.item(), score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz039G3cLEFB"
      },
      "source": [
        "def predict(model, test_x, test_y, labels_map):\r\n",
        "    '''\r\n",
        "    decoder : Model\r\n",
        "    test_x  : tensor([[12,342,...,0],[56,2311,....],....]) Example encodings\r\n",
        "    true : tensor([41,127,234,16,....]) Label Encodings\r\n",
        "\r\n",
        "    '''\r\n",
        "    out = model(test_x) # out : B x L, where B is batch size and L is number of Labels/Classes\r\n",
        "    out = out.argmax(dim=1)\r\n",
        "    correct = out.eq(test_y.data.view_as(out)).cpu().sum()\r\n",
        "    \r\n",
        "    accuracy = (correct/len(test_x))*100\r\n",
        "    print(\"Accuracy is {}%\".format(accuracy))\r\n",
        "\r\n",
        "    # Convert label encoding to labels\r\n",
        "    out = [ list(labels_map.keys())[list(labels_map.values()).index(i)] for i in out ] \r\n",
        "    test_y = [ list(labels_map.keys())[list(labels_map.values()).index(i)] for i in test_y ]\r\n",
        "    conf_matrix = confusion_matrix(y_pred=out, y_true=test_y)\r\n",
        "    \r\n",
        "    # code for heatmaps:\r\n",
        "    group_counts = [\"{0:0.0f}\\n\".format(value) for value in conf_matrix.flatten()]\r\n",
        "    # we want percentages to show % of predictions in each true cell, which means the the sums should be done by row\r\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in (conf_matrix/conf_matrix.sum(axis=1)).flatten()]\r\n",
        "    box_labels = [f\"{v1}{v2}\".strip() for v1, v2 in zip(group_counts, group_percentages)]    \r\n",
        "    box_labels = np.asarray(box_labels).reshape(conf_matrix.shape[0], conf_matrix.shape[1])\r\n",
        "    plt.subplots(figsize=(10,10))\r\n",
        "    \r\n",
        "    # PROBLEM: I have hard-coded the language group as the tick labels - ideally this should be fixed.\r\n",
        "    sns.heatmap(conf_matrix, cmap='coolwarm', annot=box_labels, fmt=\"\", linewidths=1.5, xticklabels=test_y, yticklabels=test_y)\r\n",
        "    plt.xlabel(\"Predicted Language Label\")\r\n",
        "    plt.ylabel(\"True Language Label\")\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQkFVzOlLFrS"
      },
      "source": [
        "def test_model(model, x_test, y_test, n_gram_size, vocab): \r\n",
        "    start = time.time()\r\n",
        " \r\n",
        "    print('Making', str(n_gram_size) + '-grams at {} ..'.format(time_since(start)))\r\n",
        "    x_test_grams = make_n_grams(x_test, y_test, n_gram_size)\r\n",
        " \r\n",
        "    print('Preparing encoding for Test Data at {}'.format(time_since(start))) \r\n",
        "    inp_t, true, labels_map = create_encodings(x_test_grams, y_test, vocab) \r\n",
        "    print('Encoding completed at {}'.format(time_since(start))) \r\n",
        " \r\n",
        "    return predict(model, inp_t, true, labels_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcsVA1DbAqlY"
      },
      "source": [
        "### 5. Testing and Evaluation\n",
        "\n",
        "For testing, we need to prepare our data again so that it has the required format. After the data is prepared we feed it to the model which returns a probability distribution for each example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFcw6M_hD5KL"
      },
      "source": [
        "Because of memory we have to train with smaller subsets of languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KthCnHfMn5lF"
      },
      "source": [
        "# languages with different scripts or alphabets\n",
        "diff_alph_langs = ['Arabic', 'Russian', 'Cherokee', 'Central Khmer', 'Standard Chinese', 'Japanese', \n",
        "                   'Modern Greek', 'Hebrew', 'English']\n",
        "\n",
        "# languages from different families (https://glottolog.org/glottolog/family)\n",
        "diff_fam_langs = ['Afrikaans', 'Albanian', 'Cherokee', 'Javanese', 'Hungarian', 'Turkish', \n",
        "                   'Japanese', 'Arabic', 'English']\n",
        "\n",
        "# Latin-alphabet languages with similar 4-grams (see Section 2 \"Understanding Data\")\n",
        "latin_4_grams = ['French', 'English', 'Italian', 'Portuguese', 'Estonian',\n",
        "                 'Turkish', 'Romanian', 'Swedish', 'Latin', 'Dutch']\n",
        "\n",
        "# groups of similar languages from DSL benchmark (https://arxiv.org/abs/1610.00031)\n",
        "dsl_groups_ABCG = ['Bosnian', 'Croatian', 'Serbian', # A\n",
        "                   'Indonesian', 'Malay', # B\n",
        "                   'Czech', 'Slovak', # C\n",
        "                   'Bulgarian', 'Macedonian'] # G"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUsQ7VlHRPhx"
      },
      "source": [
        "**Hyper Parameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CTS17WJRFn7"
      },
      "source": [
        "\r\n",
        "gram_log = defaultdict(dict)\r\n",
        "\r\n",
        "n_gram_list = [3,4,5,6,7,8]\r\n",
        "n_instance = 200\r\n",
        "n_feature = 10\r\n",
        "parameters = {'n_epochs':[1000,3000,5000],'hidden_size':[64,128,256],'n_layers':[1,2,3],'lr':[0.001,0.005,0.01,0.05,0.1]}\r\n",
        "X_Train, Y_Train = preprocess(X_train[:-1], Y_train[:-1])\r\n",
        "X_Test, Y_Test = preprocess(X_test[:-1], Y_test[:-1])\r\n",
        "\r\n",
        "for n_gram in n_gram_list:\r\n",
        "  param_log = defaultdict(lambda: defaultdict(list))\r\n",
        "  start = time.time()\r\n",
        "  vocab, inp, target = prepare_data(X_Train, Y_Train, n_instance, n_gram, n_feature, dsl_groups_ABC)\r\n",
        "  input_size = len(vocab)\r\n",
        "  x_test, y_test = get_data_chunk(X_Test, Y_Test, n_instance,dsl_groups_ABC)\r\n",
        "  for key,value in parameters.items():\r\n",
        "      for val in value:\r\n",
        "          args = {key:val}\r\n",
        "          model, losses, scores = start_training(inp, target, input_size, **args)\r\n",
        "          param_log[key]['losses'].append(losses)\r\n",
        "          param_log[key]['f1_score'].append(scores)\r\n",
        "          acc = test_model(model, x_test, y_test, n_gram, vocab)\r\n",
        "          param_log[key]['accuracy'].append(acc)\r\n",
        "  gram_log[n_gram] = param_log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJZOL98LI3oo"
      },
      "source": [
        "Comparison to benchmark module langdetect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgbQVJVnKhmA"
      },
      "source": [
        "#%pip install langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBno3knpI2U6"
      },
      "source": [
        "\n",
        "from langdetect import detect_langs\n",
        "from langdetect import detect\n",
        "\n",
        "def convertcode(lang, towiki=True):\n",
        "  if towiki==True:\n",
        "    lang = labels['Wiki Code'].where(labels['English'] == lang).dropna().values[0]\n",
        "  else:\n",
        "    lang = labels['English'].where(labels['Wiki Code'] == lang).dropna().values[0]\n",
        "  return lang\n",
        "\n",
        "knownlangs = ('af, ar, bg, bn, ca, cs, cy, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, kn, ko, lt, lv, mk, ml, mr, ne, nl, no, pa, pl, pt, ro, ru, sk, sl, so, sq, sv, sw, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw').split(\", \")\n",
        "#print(knownlangs)\n",
        "languages = dsl_groups_ABC\n",
        "languageswiki = [convertcode(i, towiki=True) for i in languages]\n",
        "languageswiki = list(set(languageswiki).intersection(knownlangs))\n",
        "usedlangs = [convertcode(i, towiki=False) for i in languageswiki]\n",
        "\n",
        "print(usedlangs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEE22wH-JFNl"
      },
      "source": [
        "n_instances = [200]\n",
        "ldcorrect = 0\n",
        "nlanguages = len(usedlangs)\n",
        "y_pred = []\n",
        "\n",
        "for n_instance in n_instances:\n",
        "\n",
        "  x_test, y_test = get_data_chunk(X_Test, Y_Test, n_instance, usedlangs)\n",
        "  for x, y in zip(x_test, y_test):\n",
        "    y = convertcode(y)\n",
        "    #print(x[:50], \"... \\nLanguage:\", y)\n",
        "    y_prd = detect(x)\n",
        "    #print(\"y_pred =\", detect(x))\n",
        "    if y_prd == y:\n",
        "      ldcorrect += 1\n",
        "    else:\n",
        "      print(y_prd, y)\n",
        "\n",
        "    y_pred.append(convertcode(y_prd, False))\n",
        "\n",
        "  ldaccuracy = ldcorrect/(nlanguages*n_instance)\n",
        "  print(\"Accuracy of langdetect with\",n_instance,\"paragraphs per language = \",ldaccuracy)\n",
        "  # take a paragraph/s (instance/s) at a time, feed to langdetect\n",
        "  # compare langdetect output (probabilities or best-guess language) with the true label\n",
        "  # calculate accuracy\n",
        "  # calculate confusion matrix\n",
        "  # run our RNN trained with optimised hyperparams on the same paragraphs\n",
        "  # compare accuracy and confusion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB-4DXcEJKk_"
      },
      "source": [
        "ldcf_mat = confusion_matrix(y_test, y_pred)\n",
        "print(set(y_pred))\n",
        "print(set(y_test))\n",
        "print(ldcf_mat)\n",
        "print((ldcf_mat).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIp2gsFaJLYN"
      },
      "source": [
        "\n",
        "\n",
        "# code for heatmaps:\n",
        "group_counts = [\"{0:0.0f}\\n\".format(value) for value in ldcf_mat.flatten()]\n",
        "# we want percentages to show % of predictions in each true cell, which means the the sums should be done by row\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in (ldcf_mat/ldcf_mat.sum(axis=1)).flatten()]\n",
        "box_labels = [f\"{v1}{v2}\".strip() for v1, v2 in zip(group_counts, group_percentages)]    \n",
        "box_labels = np.asarray(box_labels).reshape(ldcf_mat.shape[0], ldcf_mat.shape[1])\n",
        "plt.subplots(figsize=(10,10))\n",
        "\n",
        "sns.heatmap(ldcf_mat, cmap='coolwarm', annot=box_labels, fmt=\"\", linewidths=1.5)#, xticklabels=set(y_pred), yticklabels=set(y_pred))\n",
        "plt.xlabel(\"Predicted Language Label\")\n",
        "plt.ylabel(\"True Language Label\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}