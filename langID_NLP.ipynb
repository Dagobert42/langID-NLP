{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "langID_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dagobert42/langID-NLP/blob/main/langID_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgCjPUC4Sk86"
      },
      "source": [
        "### Dataset Preparation\n",
        "This subsection contains methods to produce uniformly distributed chunks of our data set. From these we can then obtain n-grams of different sizes. The Wikipedia Language Identification database contains txt-files of x_train and x_test for example sentences and accordingly ordered labels in y_train, y_test.\n",
        "We read these examples and cluster them by their respective language label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt1fyVMmyyc5"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import string\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from nltk import ngrams\r\n",
        "import collections\r\n",
        "from collections import defaultdict\r\n",
        "from collections import Counter"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v0_QzWx9XRU"
      },
      "source": [
        "# read data\r\n",
        "# written for the WiLI-2018 data set: https://zenodo.org/record/841984\r\n",
        "# make sure txt-files are in the specified directory when running this\r\n",
        "X_train = open('x_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_train = open('y_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "X_test = open('x_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_test = open('y_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "labels = pd.read_csv('labels.csv', delimiter = ';')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njIRm3SO9xiV"
      },
      "source": [
        "# preprocessing the data\r\n",
        "\r\n",
        "def preprocess(X,Y):\r\n",
        "\r\n",
        "  # convert language labels to language Name => 'en' -> 'English'\r\n",
        "  lab_dict = { labels.loc[i]['Label'] : labels.loc[i]['English'] for i in range(0, len(labels)) }\r\n",
        "  y_train = [ lab_dict[item] if item != 'nan' else 'Min Nan Chinese' for item in Y ]\r\n",
        "\r\n",
        "  # remove unnecessary characters from data\r\n",
        "  extras = '!\"$%&/{}[]()=?\\\\`´*+~#-_.:,;<>|1234567890°-\\'' # Characters to remove from data\r\n",
        "  rx = '[' + re.escape(''.join(extras)) + ']'\r\n",
        "  x_train = [] \r\n",
        "  to_remove = []\r\n",
        "  i = 0\r\n",
        "  for example in X:\r\n",
        "      processed = re.sub(' +', ' ', re.sub(rx, '', example))\r\n",
        "      if(len(\"\".join(processed.split()))): # Some examples after preprocessing only contain spaces, this is a check for those examples.\r\n",
        "        x_train.append(processed)\r\n",
        "      else:\r\n",
        "        y_train.pop(i)\r\n",
        "      i+=1\r\n",
        "\r\n",
        "  return x_train,y_train\r\n",
        "\r\n",
        "# x_train = [ex1,ex2,ex3,...]\r\n",
        "# y_train = [lang_of_ex1,......]"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh1WIZ_bAkZA"
      },
      "source": [
        "# sort data by language\r\n",
        "def data_by_lang(X, Y):\r\n",
        "    lang_corpora = defaultdict(list)\r\n",
        "    lang_idx = defaultdict(list)\r\n",
        "    for i in range(len(X)):\r\n",
        "        lang_corpora[Y[i]].append(X[i])\r\n",
        "        lang_idx[Y[i]].append(i)\r\n",
        "\r\n",
        "    return lang_corpora, lang_idx\r\n",
        "# lang_corpora = { 'Lang1' : [ex1,ex2,...], 'Lang2' : [ex1, ex2,,...],...}\r\n",
        "# land_idx = { 'Lang1' : [23,41,..index of example in Lang1..], 'Lang2' : [1,19,....],...}"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNBMCn3bA078"
      },
      "source": [
        "# extract uniformly distributed list of examples from our data set\r\n",
        "# takes an optional argument to constrain the list of languages\r\n",
        "def get_data_chunk(X, Y, n_instances, lang_keys=[]):\r\n",
        "    _, lang_idx = data_by_lang(X, Y)\r\n",
        "    x_train = []\r\n",
        "    y_train = []\r\n",
        "    \r\n",
        "    langs = set()\r\n",
        "    if lang_keys: \r\n",
        "        langs = set(lang_keys)\r\n",
        "    else:\r\n",
        "        langs = set(Y)\r\n",
        "\r\n",
        "    for lang in langs:\r\n",
        "        indices = lang_idx[lang]\r\n",
        "        for index in range(n_instances):\r\n",
        "            x_train.append(X[indices[index]])\r\n",
        "            y_train.append(Y[indices[index]])\r\n",
        "\r\n",
        "    return x_train, y_train\r\n",
        "\r\n",
        "# x_train [ lang1_ex,lang1_ex,..n_instance_of_lang1..,lang2_ex,lang2_ex,....,...]\r\n",
        "# y_train [ lang1,lang1,....lang2,lang2,...]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_gdP0c8ZGtJ"
      },
      "source": [
        "# creating n-grams for each data entry\r\n",
        "# optional arguments:\r\n",
        "#    lang_keys - constrains the languages to use\r\n",
        "#    stepsize  - specifies the amount of characters\r\n",
        "#                to jump until the next n-gram\r\n",
        "# returns a list of n-grams\r\n",
        "def make_n_grams(X, Y, n, lang_keys=[], stepsize=1):\r\n",
        "    assert stepsize >= 1\r\n",
        "    x_to_grams = []\r\n",
        "\r\n",
        "    langs = set()\r\n",
        "    if lang_keys: \r\n",
        "        langs = set(lang_keys)\r\n",
        "    else:\r\n",
        "        langs = set(Y)\r\n",
        "\r\n",
        "    for i in range(len(X)):\r\n",
        "        if Y[i] in langs:\r\n",
        "            sent = X[i]\r\n",
        "            x_to_grams.append([sent[j:j+n] for j in range(0, len(sent) - n+1, stepsize)])\r\n",
        "\r\n",
        "    return x_to_grams\r\n",
        "\r\n",
        "# x_to_grams = [[ngram_in_ex1],[ngram_in_ex2],....]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT5Bp8t6ZJyz"
      },
      "source": [
        "# counting and sorting n-grams for each language\r\n",
        "# returns a sorted dict of lang : {n-gram : count}\r\n",
        "def sort_by_tf(X, Y):\r\n",
        "    # calculating term frequency of n-grams per language\r\n",
        "    tf_per_lang = defaultdict(list)\r\n",
        "    langs = set(Y)\r\n",
        "    data, _ = data_by_lang(X, Y)\r\n",
        "    for lang,gram_list in data.items():\r\n",
        "      data[lang] = [ gram for grams in gram_list for gram in grams] # Comvert list of lists to a single list\r\n",
        "    for lang in langs:\r\n",
        "        tf_per_lang[lang] = dict(\r\n",
        "            zip(list(Counter(data[lang]).keys()),\r\n",
        "                 list(Counter(data[lang]).values())))\r\n",
        "\r\n",
        "    # sort by term frequency\r\n",
        "    sorted_tf_per_lang = defaultdict(list)\r\n",
        "    for lang in langs:\r\n",
        "        sorted_tf_per_lang[lang] = { word : value for word, value in sorted(tf_per_lang[lang].items(), key=lambda item:item[1], reverse=True) }\r\n",
        "    \r\n",
        "    return sorted_tf_per_lang\r\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3cYY3GVpMy"
      },
      "source": [
        "### Understanding Data\n",
        "In the following we review some examples to get an understanding of our data...\n",
        "Particularly interesting are languages with a degree of similarity. Here we print examples of languages that use the latin alphabet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUM_YDQ4dqkD"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC_uPl7fMbOf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c65263ec-fad7-4736-be23-fed1b875ef60"
      },
      "source": [
        "'''\n",
        "x_train, y_train = preprocess(X_train[:-1],Y_train[:-1])\n",
        "\n",
        "lang_corpora, lang_idx = data_by_lang(x_train, y_train)\n",
        "# produce charts of counts of common n-grams in different languages, and a table suggesting similar languages based on these\n",
        "\n",
        "for n_gram_size in range(3, 6):\n",
        "    m_samples = 20\n",
        "    latin_languages = ['German', 'English', 'French', 'Spanish', 'Italian', 'Portuguese', \n",
        "                       'Estonian', 'Turkish', 'Romanian', 'Swedish', 'Latin', 'Dutch']\n",
        "\n",
        "    ng_related = {}\n",
        "    \n",
        "    x_train_grams = make_n_grams(x_train, y_train, n_gram_size, latin_languages)\n",
        "    sorted_tf_per_lang = sort_by_tf(x_train_grams, y_train)\n",
        "\n",
        "    for lang_key in latin_languages:\n",
        "        ng_related[lang_key] = []\n",
        "        latin_languages.remove(lang_key)\n",
        "        latin_langs = latin_languages\n",
        "        for otherlang in latin_langs:\n",
        "            top_m = list(sorted_tf_per_lang[lang_key].keys())[:m_samples]\n",
        "            top_m_x = list(sorted_tf_per_lang[otherlang].keys())[:m_samples]\n",
        "\n",
        "            # compares the two top m lists for common elements:\n",
        "            common_ngrams = list(set(top_m).intersection(top_m_x))\n",
        "                \n",
        "            if len(common_ngrams) > 3: # if two languages share 4 or more n-grams in their top n n-grams\n",
        "                \n",
        "                print(lang_key, \"and\", otherlang, \"have the following frequent\", n_gram_size,\"-grams in common:\",common_ngrams)\n",
        "                ng_related[lang_key].append(otherlang)\n",
        "                \n",
        "            # find counts of the entries in common_ngrams for each language.\n",
        "            # These are stored as the values corresponding to the ngram keys in the dictionary\n",
        "\n",
        "                counts_langkey = []\n",
        "                counts_otherlang = []\n",
        "                for i in common_ngrams:\n",
        "                    counts_langkey.append(sorted_tf_per_lang[lang_key][i])\n",
        "                    counts_otherlang.append(sorted_tf_per_lang[otherlang][i])\n",
        "\n",
        "                common_ngrams = [k.replace(' ', '_') for k in common_ngrams]\n",
        "\n",
        "                # code for bar chart:\n",
        "                x = np.arange(len(common_ngrams))  # the label locations\n",
        "                width = 0.35  # the width of the bars\n",
        "\n",
        "                fig, ax = plt.subplots()\n",
        "                rects1 = ax.bar(x - width/2, counts_langkey, width, color = 'r', label=lang_key)\n",
        "                rects2 = ax.bar(x + width/2, counts_otherlang, width, color = 'g', label=otherlang)\n",
        "                ax.set_ylabel('Count')\n",
        "                ax.set_title('Frequency of %s-grams in given languages' % (n_gram_size))\n",
        "                ax.set_xticks(x)\n",
        "                ax.set_xticklabels(common_ngrams, fontsize=12)\n",
        "                ax.legend()\n",
        "                fig.tight_layout()\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "                #counts_langkey = []\n",
        "                #counts_otherlang = []\n",
        "\n",
        "        print('\\n ')                 \n",
        "    \n",
        "    print('similar languages based on ', n_gram_size, '- grams:')\n",
        "    for key, val in ng_related.items():\n",
        "        print(key, ':', val)\n",
        "    print('\\n ')\n",
        "'''"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nx_train, y_train = preprocess(X_train[:-1],Y_train[:-1])\\n\\nlang_corpora, lang_idx = data_by_lang(x_train, y_train)\\n# produce charts of counts of common n-grams in different languages, and a table suggesting similar languages based on these\\n\\nfor n_gram_size in range(3, 6):\\n    m_samples = 20\\n    latin_languages = [\\'German\\', \\'English\\', \\'French\\', \\'Spanish\\', \\'Italian\\', \\'Portuguese\\', \\n                       \\'Estonian\\', \\'Turkish\\', \\'Romanian\\', \\'Swedish\\', \\'Latin\\', \\'Dutch\\']\\n\\n    ng_related = {}\\n    \\n    x_train_grams = make_n_grams(x_train, y_train, n_gram_size, latin_languages)\\n    sorted_tf_per_lang = sort_by_tf(x_train_grams, y_train)\\n\\n    for lang_key in latin_languages:\\n        ng_related[lang_key] = []\\n        latin_languages.remove(lang_key)\\n        latin_langs = latin_languages\\n        for otherlang in latin_langs:\\n            top_m = list(sorted_tf_per_lang[lang_key].keys())[:m_samples]\\n            top_m_x = list(sorted_tf_per_lang[otherlang].keys())[:m_samples]\\n\\n            # compares the two top m lists for common elements:\\n            common_ngrams = list(set(top_m).intersection(top_m_x))\\n                \\n            if len(common_ngrams) > 3: # if two languages share 4 or more n-grams in their top n n-grams\\n                \\n                print(lang_key, \"and\", otherlang, \"have the following frequent\", n_gram_size,\"-grams in common:\",common_ngrams)\\n                ng_related[lang_key].append(otherlang)\\n                \\n            # find counts of the entries in common_ngrams for each language.\\n            # These are stored as the values corresponding to the ngram keys in the dictionary\\n\\n                counts_langkey = []\\n                counts_otherlang = []\\n                for i in common_ngrams:\\n                    counts_langkey.append(sorted_tf_per_lang[lang_key][i])\\n                    counts_otherlang.append(sorted_tf_per_lang[otherlang][i])\\n\\n                common_ngrams = [k.replace(\\' \\', \\'_\\') for k in common_ngrams]\\n\\n                # code for bar chart:\\n                x = np.arange(len(common_ngrams))  # the label locations\\n                width = 0.35  # the width of the bars\\n\\n                fig, ax = plt.subplots()\\n                rects1 = ax.bar(x - width/2, counts_langkey, width, color = \\'r\\', label=lang_key)\\n                rects2 = ax.bar(x + width/2, counts_otherlang, width, color = \\'g\\', label=otherlang)\\n                ax.set_ylabel(\\'Count\\')\\n                ax.set_title(\\'Frequency of %s-grams in given languages\\' % (n_gram_size))\\n                ax.set_xticks(x)\\n                ax.set_xticklabels(common_ngrams, fontsize=12)\\n                ax.legend()\\n                fig.tight_layout()\\n\\n                plt.show()\\n\\n                #counts_langkey = []\\n                #counts_otherlang = []\\n\\n        print(\\'\\n \\')                 \\n    \\n    print(\\'similar languages based on \\', n_gram_size, \\'- grams:\\')\\n    for key, val in ng_related.items():\\n        print(key, \\':\\', val)\\n    print(\\'\\n \\')\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3vFh28hYXfA"
      },
      "source": [
        "### Naive Bayes Classifier\n",
        "To obtain a baseline for the language identification task we employ a simple Naive Bayes classifier. Our first step is to collect the top n-grams into feature matrices..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlUCSGN9Wg3l"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import time,math"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07I5K2SxKXWF"
      },
      "source": [
        "def time_since(since):\r\n",
        "    s = time.time() - since\r\n",
        "    m = math.floor(s / 60)\r\n",
        "    s -= m * 60\r\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stiyBC09ePY0"
      },
      "source": [
        "# extracts lists of top n frequent n-grams from data\r\n",
        "def get_top_n_features(X_grams, Y, n_features):\r\n",
        "  '''\r\n",
        "  X : [[ngram_in_ex1],[ngram_in_ex2],....]\r\n",
        "  Y : ['lang1','lang1',...,'lang2',...]\r\n",
        "  n_features : number of ngram to pick from each language\r\n",
        "  '''\r\n",
        "  sorted_freq_per_lang = sort_by_tf(X_grams, Y)\r\n",
        "  features = []\r\n",
        "  for lang, grams_dict in sorted_freq_per_lang.items():\r\n",
        "      i = 0\r\n",
        "      for gram, count in grams_dict.items():\r\n",
        "          if i <= n_features:\r\n",
        "              features.append(gram)\r\n",
        "          else:\r\n",
        "              break\r\n",
        "          i += 1\r\n",
        "      \r\n",
        "  return list(set(features))\r\n",
        "\r\n",
        "# features : ['and','he ','öä ',....] Top ngrams from corpus"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUr77bPVeVQm"
      },
      "source": [
        "# convert data to feature matrix\r\n",
        "def create_feature_matrix(X, features):\r\n",
        "  '''\r\n",
        "  X : [[ngram_in_ex1],[ngram_in_ex2],....]\r\n",
        "  features : ['and','he ','öä ',....] Top ngrams from corpus\r\n",
        "\r\n",
        "  '''\r\n",
        "  mat = np.zeros((len(X),len(features)))\r\n",
        "  i = 0\r\n",
        "  for gram_list in X:\r\n",
        "      gram_count = []\r\n",
        "      for gram in features:          \r\n",
        "          if gram in gram_list:\r\n",
        "              gram_count.append(gram_list.count(gram)+1)\r\n",
        "          else:\r\n",
        "              gram_count.append(1)\r\n",
        "      mat[i] = gram_count\r\n",
        "      i+=1\r\n",
        "\r\n",
        "  return mat\r\n",
        "  # mat : array([[4,1,2,1,...],[1,1,1,2,3,1,...],...])"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaeJeeEvRtDd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "df2c0ff4-a7dd-446d-8ed9-2994ce9e70bc"
      },
      "source": [
        "'''\r\n",
        "n_instances = 100 # instance per language\r\n",
        "n_gram_size = 5\r\n",
        "n_features = 10 # features per language\r\n",
        "start = time.time()\r\n",
        "print(\"Starting preprocessing at {} ..\".format(time_since(start)))\r\n",
        "\r\n",
        "x_train, y_train = preprocess(X_train[:-1], Y_train[:-1])\r\n",
        "\r\n",
        "print(\"Preprocessing Done at {}.\".format(time_since(start)))\r\n",
        "\r\n",
        "# reduce languages to get smaller data subset\r\n",
        "x_train, y_train = get_data_chunk(x_train, y_train, n_instances)\r\n",
        "\r\n",
        "print(\"Making ngrams at {} ..\".format(time_since(start)))\r\n",
        "x_train_grams = make_n_grams(x_train, y_train, n_gram_size)\r\n",
        "\r\n",
        "print('Extracting features at {} ...'.format(time_since(start)))\r\n",
        "# create features for dataset\r\n",
        "features = get_top_n_features(x_train_grams, y_train, n_features)\r\n",
        "\r\n",
        "print('Creating feature matrix at {}  ....'.format(time_since(start)))\r\n",
        "# convert dataset into feature matrix\r\n",
        "feature_matrix = create_feature_matrix(x_train_grams, features)\r\n",
        "print('Data Preperation completed after {}'.format(time_since(start)))\r\n",
        "'''"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nn_instances = 100 # instance per language\\nn_gram_size = 5\\nn_features = 10 # features per language\\nstart = time.time()\\nprint(\"Starting preprocessing at {} ..\".format(time_since(start)))\\n\\nx_train, y_train = preprocess(X_train[:-1], Y_train[:-1])\\n\\nprint(\"Preprocessing Done at {}.\".format(time_since(start)))\\n\\n# reduce languages to get smaller data subset\\nx_train, y_train = get_data_chunk(x_train, y_train, n_instances)\\n\\nprint(\"Making ngrams at {} ..\".format(time_since(start)))\\nx_train_grams = make_n_grams(x_train, y_train, n_gram_size)\\n\\nprint(\\'Extracting features at {} ...\\'.format(time_since(start)))\\n# create features for dataset\\nfeatures = get_top_n_features(x_train_grams, y_train, n_features)\\n\\nprint(\\'Creating feature matrix at {}  ....\\'.format(time_since(start)))\\n# convert dataset into feature matrix\\nfeature_matrix = create_feature_matrix(x_train_grams, features)\\nprint(\\'Data Preperation completed after {}\\'.format(time_since(start)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsIdn1iIRu2g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "015c9096-75d2-4b18-d4bf-e440cd991226"
      },
      "source": [
        "'''\r\n",
        "# Gaussian Naive Bayes Model Training\r\n",
        "encoder = LabelEncoder()\r\n",
        "X = feature_matrix\r\n",
        "Y = encoder.fit_transform(y_train)\r\n",
        "model = GaussianNB()\r\n",
        "model.fit(X,Y)\r\n",
        "'''\r\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Gaussian Naive Bayes Model Training\\nencoder = LabelEncoder()\\nX = feature_matrix\\nY = encoder.fit_transform(y_train)\\nmodel = GaussianNB()\\nmodel.fit(X,Y)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjTptZN9Rxxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "outputId": "5fdad0b6-b7e2-4f9f-d256-ece6826a27dd"
      },
      "source": [
        "'''\r\n",
        "# model testing\r\n",
        "start = time.time()\r\n",
        "print('Test Data preperation starting ...')\r\n",
        "x,y = preprocess(X_test[:20000], Y_test[:20000])\r\n",
        "\r\n",
        "print(\"Making ngrams at {} ..\".format(time_since(start)))\r\n",
        "x_test_grams = make_n_grams(x, y, n_gram_size)\r\n",
        "\r\n",
        "print('Creating feature matrix at {}  ....'.format(time_since(start)))\r\n",
        "x = create_feature_matrix(x_test_grams, features)\r\n",
        "\r\n",
        "print('Test Data Preperation completed after {}'.format(time_since(start)))\r\n",
        "\r\n",
        "y = encoder.fit_transform(y)\r\n",
        "y_pred = model.predict(x)\r\n",
        "conf_matrix = confusion_matrix(y_pred=y_pred, y_true=y)\r\n",
        "acc = round(accuracy_score(y_pred=y_pred, y_true=y), 4) * 100\r\n",
        "print(f\"Accuracy is {acc}%\")\r\n",
        "'''\r\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# model testing\\nstart = time.time()\\nprint(\\'Test Data preperation starting ...\\')\\nx,y = preprocess(X_test[:20000], Y_test[:20000])\\n\\nprint(\"Making ngrams at {} ..\".format(time_since(start)))\\nx_test_grams = make_n_grams(x, y, n_gram_size)\\n\\nprint(\\'Creating feature matrix at {}  ....\\'.format(time_since(start)))\\nx = create_feature_matrix(x_test_grams, features)\\n\\nprint(\\'Test Data Preperation completed after {}\\'.format(time_since(start)))\\n\\ny = encoder.fit_transform(y)\\ny_pred = model.predict(x)\\nconf_matrix = confusion_matrix(y_pred=y_pred, y_true=y)\\nacc = round(accuracy_score(y_pred=y_pred, y_true=y), 4) * 100\\nprint(f\"Accuracy is {acc}%\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppf3M1lWS9BS"
      },
      "source": [
        "# Model                Instance_per_language   N_gram       Features_per_language         Accuracy             Test_Instance\r\n",
        "# GaussianNB              150                     3                   40                     79%                   20k\r\n",
        "# GaussianNB              150                     4                   40                     87%                   20k\r\n",
        "# GaussianNB              150                     5                   40                     87%                   25k\r\n",
        "# GaussianNB              200                     5                   30                     85%                   25k\r\n",
        "# MultinomialNB           150                     3                   40                     77%                   25k  \r\n",
        "# MultinomialNB           150                     4                   40                     73%                   25k  "
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC1mI32qeqY8"
      },
      "source": [
        "### RNN Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3fZ8Vrmd5hi"
      },
      "source": [
        "import torch\r\n",
        "import time\r\n",
        "from torch.autograd import Variable \r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmHu-KycMevY"
      },
      "source": [
        "**CHANGE TO GPU MODE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFTaWPSMdqO"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPUswhM1qH8X"
      },
      "source": [
        "class RNN(nn.Module):\r\n",
        "    def __init__(self, input_size, hid_size, output_size, layers, embedding):\r\n",
        "\r\n",
        "        super(RNN, self).__init__()\r\n",
        "        self.hidden_dim = hid_size\r\n",
        "        self.layers = layers\r\n",
        "        self.embedding_size = embedding \r\n",
        "        self.dropout = nn.Dropout(0.4)\r\n",
        "        self.input_size = input_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.embeddings = nn.Embedding(self.input_size,self.embedding_size)\r\n",
        "        self.rnn = nn.GRU(input_size=self.embedding_size,hidden_size=self.hidden_dim,num_layers = self.layers)\r\n",
        "        self.linear = nn.Linear(self.hidden_dim,self.output_size)\r\n",
        "\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        # x : B x S where B is batch size and S is sequence size\r\n",
        "        # Sequence size is length of one ngram vector encoding\r\n",
        "        batch_size = x.size(0) \r\n",
        "        x = x.t() \r\n",
        "        embedded = self.embeddings(x) # S x B x I , here I is input_size/vocab size\r\n",
        "        hidden = self._init_hidden(batch_size)\r\n",
        "        output,hidden = self.rnn(embedded,hidden)\r\n",
        "        output = self.dropout(output) \r\n",
        "        fc_output = self.linear(output[-1]) # B x L , L is number of classes/languages\r\n",
        "        return fc_output\r\n",
        "\r\n",
        "    def _init_hidden(self,batch_size):\r\n",
        "        hidden_state = torch.zeros(self.layers,batch_size, self.hidden_dim, device=device)\r\n",
        "        return hidden_state"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ennmQVVXqfI4"
      },
      "source": [
        "def padding(vector_inps, lengths):\r\n",
        "    '''\r\n",
        "    This function takes variable lengths vectors and convert them into equal length by padding 0\r\n",
        "    Input : \"vector_inps\" list of vectors containing indices of ngrams\r\n",
        "            \"lengths \" length of each vector\r\n",
        "    Output : tensor containing vectors of equal length after padding. This length is equal to maximum number(M) in list of \"lengths\".\r\n",
        "    '''\r\n",
        "    inp_tensor = torch.zeros((len(vector_inps),lengths.max()), device= device).long()\r\n",
        "    for idx, (seq, seq_len) in enumerate(zip(vector_inps,lengths)):\r\n",
        "        inp_tensor[idx, :seq_len] = torch.LongTensor(seq)\r\n",
        "    return inp_tensor\r\n",
        "\r\n",
        "    # inp_tensor : tensor([[12,342,...,0],[56,2311,....],....])"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU5wFzymqguQ"
      },
      "source": [
        "def train(decoder, criterion, decoder_optimizer, inp, target, batch = 100):\r\n",
        "    '''\r\n",
        "    decoder : Model\r\n",
        "    inp  : tensor([[12,342,...,0],[56,2311,....],....]) Example encodings\r\n",
        "    target : tensor([41,127,234,16,....]) Label Encodings\r\n",
        "    '''\r\n",
        "    decoder.zero_grad()\r\n",
        "    loss = 0\r\n",
        "    for i in range(1,int(len(inp)/batch)):\r\n",
        "        # input to model should be in form B x S where B is batch size and S is sequence size\r\n",
        "        output = decoder(inp[(i-1) * batch: (i) * batch].view(batch, -1))\r\n",
        "        loss += criterion(output, target[(i-1)*batch:(i)*batch])\r\n",
        "    loss.backward()\r\n",
        "    decoder_optimizer.step()\r\n",
        "\r\n",
        "    return loss.item()"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRlIup7rK5RY"
      },
      "source": [
        "def create_encodings(X_grams, Y, word_to_ix):\r\n",
        "    '''\r\n",
        "    X_grams : [[ngram_in_ex1],[ngram_in_ex2],....]\r\n",
        "    Y : ['lang1','lang1',...,'lang2',....]\r\n",
        "    word_to_ix : {' of':1,'apf':2,....} This is vocabulary of selected top ngrams\r\n",
        "    '''\r\n",
        "    x_grams = []\r\n",
        "    y_grams = []\r\n",
        "    gram_len = []\r\n",
        "    iter = len(X_grams)/10\r\n",
        "    print('Creating Encoding for X')\r\n",
        "    for j in range(len(X_grams)):\r\n",
        "        gramlist = X_grams[j] # list of ngrams in example j\r\n",
        "        gramlist = list(dict.fromkeys(sorted(gramlist,key=gramlist.count,reverse=True)))\r\n",
        "        grams = [word_to_ix[w] for w in gramlist if w in list(word_to_ix.keys()) ] \r\n",
        "        \r\n",
        "        if(len(grams) >= 1): # resulting grams list must not be empty\r\n",
        "            x_grams.append(grams) # Add encodings to x_grams\r\n",
        "            gram_len.append(len(grams)) # Add corresponding data\r\n",
        "            y_grams.append(Y[j])\r\n",
        "\r\n",
        "        if( j % iter == 0):\r\n",
        "            print(\"{}% data prepared at time {}\".format((j/len(X_grams))*100,time_since(start)))\r\n",
        "\r\n",
        "    gram_len = torch.LongTensor(gram_len)\r\n",
        "    inp = padding(x_grams,gram_len)\r\n",
        "\r\n",
        "    print('Creating Encoding for Y')\r\n",
        "    label = list(set(y_grams))\r\n",
        "    labels_to_idx = { lang:i  for i,lang in enumerate(label)}\r\n",
        "    y_label = torch.zeros(len(y_grams),device=device).long()\r\n",
        "    for i in range(len(y_label)):\r\n",
        "        y_label[i] = labels_to_idx[y_grams[i]]\r\n",
        "    target = Variable(torch.cuda.LongTensor(y_label))\r\n",
        "    return inp, target\r\n",
        "\r\n",
        "  # inp  : tensor([[12,342,...,0],[56,2311,....],....])\r\n",
        "  # target : tensor([41,127,234,16,....])"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlTIy_zRkxNE"
      },
      "source": [
        "def prepare_data(X, Y, n_samples, n_gram_size, n_features, lang_keys=[]):\r\n",
        "    '''\r\n",
        "    Data is prepared in 3 steps:\r\n",
        "    1. ngram is created from given dataset i.e each example is converted into a list of ngram\r\n",
        "    2. These ngrams are used to create vocabulary\r\n",
        "    3. Finally dataset is converted into encodings to feed into RNN\r\n",
        "    '''\r\n",
        "\r\n",
        "    # reduce languages to get smaller data subset\r\n",
        "    x_train, y_train = get_data_chunk(X, Y, n_samples, lang_keys)\r\n",
        "\r\n",
        "    print(\"Making n-grams at {} ..\".format(time_since(start)), 'of size: ', n_gram_size)\r\n",
        "    x_train_grams = make_n_grams(x_train, y_train, n_gram_size, lang_keys)\r\n",
        "\r\n",
        "    # create features for dataset\r\n",
        "    print('Extracting features at {} ...'.format(time_since(start)))\r\n",
        "    features = get_top_n_features(x_train_grams, y_train, n_features)\r\n",
        "    word_to_ix = {word: i for i, word in enumerate(features)}\r\n",
        "\r\n",
        "    print('Preparing encoding at {}'.format(time_since(start)))\r\n",
        "    inp, target = create_encodings(x_train_grams, y_train, word_to_ix)\r\n",
        "    print('Encoding completed at {}'.format(time_since(start)))\r\n",
        "    return word_to_ix, inp, target"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snUkxMh3vGjJ"
      },
      "source": [
        "def start_training(X, Y, input_size, hidden_size, n_layers, embedding_size, lr, batch, n_epochs):\r\n",
        "    output_size = len(set(Y)) # number of languages\r\n",
        "    model = RNN(input_size, hidden_size, output_size, n_layers, embedding_size).to(device)\r\n",
        "    decoder_optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    print_every = n_epochs/10\r\n",
        "    all_losses = []\r\n",
        "    for epoch in range(1, n_epochs+1):\r\n",
        "        loss = train(model, criterion, decoder_optimizer, X, target, batch)\r\n",
        "        all_losses.append(loss)\r\n",
        "        if(epoch % print_every == 0):\r\n",
        "            print('[{} ({} {}%) {:.4f}]'.format(time_since(start), epoch, epoch/n_epochs * 100, loss))\r\n",
        "        \r\n",
        "    print(\"Training Complete.\")\r\n",
        "    return model, all_losses"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz039G3cLEFB"
      },
      "source": [
        "def predict(model, test_x, test_y):\r\n",
        "    '''\r\n",
        "    decoder : Model\r\n",
        "    test_x  : tensor([[12,342,...,0],[56,2311,....],....]) Example encodings\r\n",
        "    true : tensor([41,127,234,16,....]) Label Encodings\r\n",
        "\r\n",
        "    '''\r\n",
        "    out = model(test_x) # out : B x L, where B is batch size and L is number of Labels/Classes\r\n",
        "    out = out.argmax(dim=1)\r\n",
        "    correct = out.eq(test_y.data.view_as(out)).cpu().sum()\r\n",
        "    print(correct)\r\n",
        "    accuracy = (correct/len(test_x))*100\r\n",
        "    print(\"Accuracy is {}%\".format(accuracy))\r\n",
        "    \r\n",
        "    return accuracy"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQkFVzOlLFrS"
      },
      "source": [
        "def test_model(model, x_test, y_test, n_gram_size, vocab): \r\n",
        "    start = time.time()\r\n",
        "    print('Test data preparation starting ...') \r\n",
        "    x, y = preprocess(x_test, y_test) \r\n",
        " \r\n",
        "    print(\"Making n-grams at {} ..\".format(time_since(start)), 'of size: ', n_gram_size)\r\n",
        "    x_test_grams = make_n_grams(x, y, n_gram_size)\r\n",
        " \r\n",
        "    print('Preparing encoding at {}'.format(time_since(start))) \r\n",
        "    inp_t, true = create_encodings(x_test_grams, y, vocab) \r\n",
        "    print('Encoding completed at {}'.format(time_since(start))) \r\n",
        " \r\n",
        "    return predict(model, inp_t, true)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KthCnHfMn5lF"
      },
      "source": [
        "# languages with different scripts or alphabets\n",
        "diff_alph_langs = ['Arabic', 'Russian', 'Cherokee', 'Khmer', 'Standard Chinese', 'Japanese', \n",
        "                   'Modern Greek', 'Hebrew', 'English']\n",
        "\n",
        "# languages from different families (https://glottolog.org/glottolog/family)\n",
        "diff_fam_langs = ['Afrikaans', 'Albanian', 'Cherokee', 'Javanese', 'Hungarian', 'Turkish', \n",
        "                   'Japanese', 'Arabic', 'English']\n",
        "\n",
        "######### similar language sets\n",
        "# latin languages with similar 4-grams\n",
        "latin_4_grams = ['French', 'English', 'Italian', 'Portuguese', 'Estonian',\n",
        "                 'Turkish', 'Romanian', 'Swedish', 'Latin', 'Dutch']\n",
        "\n",
        "# groups of similar languages from DSL benchmark\n",
        "dsl_groups_ABC = ['Bosnian', 'Croatian', 'Serbian', 'Indonesian', 'Malay', 'Czech', 'Slovak']"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NTq9-KewZwg"
      },
      "source": [
        "# Single NGRAM as Feature\n",
        "n_gram_list = [5, 6]\n",
        "n_instances = [200, 75]\n",
        "n_features = [7, 10, 15]\n",
        "acuuracy_list = []\n",
        "loss_list = []\n",
        "\n",
        "hidden_list = [32, 64, 128, 256]\n",
        "n_layers_list = [1, 2, 3, 4]\n",
        "embedding_list = [32, 64, 128, 256]\n",
        "lr_list = [0.001, 0.005, 0.01, 0.05, 0.1,0.5]\n",
        "epoch_list = [100, 500, 1000, 3000, 5000]\n",
        "\n",
        "X_Train, Y_Train = preprocess(X_train[:-1], Y_train[:-1])\n",
        "X_Test, Y_Test = preprocess(X_test[:-1], Y_test[:-1])\n",
        "models = []\n",
        "all_losses = []\n",
        "all_accuracies = []\n",
        "\n",
        "for n_instance in n_instances:\n",
        "  for n_feature in n_features:\n",
        "    for n_gram_size in n_gram_list:\n",
        "      start = time.time()\n",
        "      vocab, inp, target = prepare_data(X_Train, Y_Train, n_instance, n_gram_size, n_feature, diff_fam_langs)\n",
        "      print('Training Phase..')\n",
        "      hidden_size = 32\n",
        "      input_size = len(vocab) # Number of grams in vocabulary\n",
        "      n_layers = 1 # Number of layers of RNN\n",
        "      embedding_size = 32\n",
        "      lr = 0.001\n",
        "      batch = 32\n",
        "      n_epochs = 1000\n",
        "      model, losses = start_training(inp, target, input_size, hidden_size, n_layers, embedding_size, lr, batch, n_epochs)\n",
        "      models.append(model)\n",
        "      all_losses.append(losses)\n",
        "\n",
        "      x_test, y_test = get_data_chunk(X_Test, Y_Test, n_instance)\n",
        "      all_accuracies.append(test_model(model, x_test, y_test, n_gram_size, vocab))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}