{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of langID_NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgCjPUC4Sk86"
      },
      "source": [
        "### Dataset Preparation\n",
        "This subsection contains methods to produce uniformly distributed chunks of our data set. From these we can then obtain n-grams of different sizes. The Wikipedia Language Identification database contains txt-files of x_train and x_test for example sentences and accordingly ordered labels in y_train, y_test.\n",
        "We read these examples and cluster them by their respective language label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt1fyVMmyyc5"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import string\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from nltk import ngrams\r\n",
        "import collections\r\n",
        "from collections import defaultdict\r\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v0_QzWx9XRU"
      },
      "source": [
        "# read data\r\n",
        "# written for the WiLI-2018 data set: https://zenodo.org/record/841984\r\n",
        "# make sure txt-files are in the specified directory when running this\r\n",
        "X_train = open('x_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_train = open('y_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "X_test = open('x_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_test = open('y_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "labels = pd.read_csv('labels.csv', delimiter = ';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njIRm3SO9xiV"
      },
      "source": [
        "# Preprocessing the data\r\n",
        "\r\n",
        "def preprocess(X,Y):\r\n",
        "\r\n",
        "  # convert language labels to language Name => 'en' -> 'English'\r\n",
        "  lab_dict = { labels.loc[i]['Label'] : labels.loc[i]['English'] for i in range(0, len(labels)) }\r\n",
        "  y_train = [ lab_dict[item] if item != 'nan' else 'Min Nan Chinese' for item in Y ]\r\n",
        "\r\n",
        "  # remove unnecessary characters from data\r\n",
        "  extras = '!\"$%&/{}[]()=?\\\\`´*+~#-_.:,;<>|1234567890°-\\'' # Characters to remove from data\r\n",
        "  rx = '[' + re.escape(''.join(extras)) + ']'\r\n",
        "  x_train = [] \r\n",
        "  to_remove = []\r\n",
        "  i = 0\r\n",
        "  for example in X:\r\n",
        "      processed = re.sub(' +', ' ', re.sub(rx, '', example))\r\n",
        "      if(len(\"\".join(processed.split()))): # Some examples after preprocessing only contain spaces, this is a check for those examples.\r\n",
        "        x_train.append(processed)\r\n",
        "      else:\r\n",
        "        y_train.pop(i)\r\n",
        "      i+=1\r\n",
        "\r\n",
        "  return x_train,y_train\r\n",
        "\r\n",
        "# x_train = [ex1,ex2,ex3,...]\r\n",
        "# y_train = [lang_of_ex1,......]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh1WIZ_bAkZA"
      },
      "source": [
        "# sort data by language\r\n",
        "def data_by_lang(X, Y):\r\n",
        "    lang_corpora = defaultdict(list)\r\n",
        "    lang_idx = defaultdict(list)\r\n",
        "    for i in range(len(X)):\r\n",
        "        lang_corpora[Y[i]].append(X[i])\r\n",
        "        lang_idx[Y[i]].append(i)\r\n",
        "\r\n",
        "    return lang_corpora, lang_idx\r\n",
        "# lang_corpora = { 'Lang1' : [ex1,ex2,...], 'Lang2' : [ex1, ex2,,...],...}\r\n",
        "# land_idx = { 'Lang1' : [23,41,..index of example in Lang1..], 'Lang2' : [1,19,....],...}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNBMCn3bA078"
      },
      "source": [
        "# extract uniformly distributed list of examples from our data set\r\n",
        "# takes an optional argument to constrain the list of languages\r\n",
        "def get_data_chunk(X, Y, n_instances, lang_keys=[]):\r\n",
        "    _, lang_idx = data_by_lang(X, Y)\r\n",
        "    x_train = []\r\n",
        "    y_train = []\r\n",
        "    \r\n",
        "    langs = set()\r\n",
        "    if lang_keys: \r\n",
        "        langs = set(lang_keys)\r\n",
        "    else:\r\n",
        "        langs = set(Y)\r\n",
        "\r\n",
        "    for lang in langs:\r\n",
        "        indices = lang_idx[lang]\r\n",
        "        for index in range(n_instances):\r\n",
        "            x_train.append(X[indices[index]])\r\n",
        "            y_train.append(Y[indices[index]])\r\n",
        "\r\n",
        "    return x_train, y_train\r\n",
        "\r\n",
        "# x_train [ lang1_ex,lang1_ex,..n_instance_of_Lang1..,lang2_ex,lang2_ex,....,...]\r\n",
        "# y_train [ lang1,lang1,....lang2,lang2,...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_gdP0c8ZGtJ"
      },
      "source": [
        "# creating n-grams for each data entry\r\n",
        "# optional arguments:\r\n",
        "#    lang_keys - constrains the languages to use\r\n",
        "#    stepsize  - specifies the amount of characters\r\n",
        "#                to jump until the next n-gram\r\n",
        "# returns a list of n-grams\r\n",
        "def make_n_grams(X, Y, n, lang_keys=[], stepsize=1):\r\n",
        "    assert stepsize >= 1\r\n",
        "    x_to_grams = []\r\n",
        "\r\n",
        "    langs = set()\r\n",
        "    if lang_keys: \r\n",
        "        langs = set(lang_keys)\r\n",
        "    else:\r\n",
        "        langs = set(Y)\r\n",
        "\r\n",
        "    for i in range(len(X)):\r\n",
        "        if Y[i] in langs:\r\n",
        "            sent = X[i]\r\n",
        "            x_to_grams.append([sent[j:j+n] for j in range(0, len(sent) - n+1, stepsize)])\r\n",
        "\r\n",
        "    return x_to_grams\r\n",
        "\r\n",
        "# x_to_grams = [[ngram_in_ex1],[ngram_in_ex2],....]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT5Bp8t6ZJyz"
      },
      "source": [
        "# counting and sorting n-grams for each language\r\n",
        "# returns a sorted dict of lang : {n-gram : count}\r\n",
        "def sort_by_tf(X, Y):\r\n",
        "    # calculating term frequency of n-grams per language\r\n",
        "    tf_per_lang = defaultdict(list)\r\n",
        "    langs = set(Y)\r\n",
        "    data, _ = data_by_lang(X, Y)\r\n",
        "    for lang,gram_list in data.items():\r\n",
        "      data[lang] = [ gram for grams in gram_list for gram in grams] # Comvert list of lists to a single list\r\n",
        "    for lang in langs:\r\n",
        "        tf_per_lang[lang] = dict(\r\n",
        "            zip(list(Counter(data[lang]).keys()),\r\n",
        "                 list(Counter(data[lang]).values())))\r\n",
        "\r\n",
        "    # sort by term frequency\r\n",
        "    sorted_tf_per_lang = defaultdict(list)\r\n",
        "    for lang in langs:\r\n",
        "        sorted_tf_per_lang[lang] = { word : value for word, value in sorted(tf_per_lang[lang].items(), key=lambda item:item[1], reverse=True) }\r\n",
        "    \r\n",
        "    return sorted_tf_per_lang\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3cYY3GVpMy"
      },
      "source": [
        "### Understanding Data\n",
        "In the following we review some examples to get an understanding of our data...\n",
        "Particularly interesting are languages with a degree of similarity. Here we print examples of languages that use the latin alphabet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUM_YDQ4dqkD"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC_uPl7fMbOf"
      },
      "source": [
        "x_train, y_train = preprocess(X_train[:-1],Y_train[:-1])\n",
        "\n",
        "lang_corpora, lang_idx = data_by_lang(x_train, y_train)\n",
        "# produce charts of counts of common n-grams in different languages, and a table suggesting similar languages based on these\n",
        "\n",
        "for n_gram_size in range(3, 6):\n",
        "    m_samples = 20\n",
        "    latin_languages = ['German', 'English', 'French', 'Spanish', 'Italian', 'Portuguese', \n",
        "                       'Estonian', 'Turkish', 'Romanian', 'Swedish', 'Latin', 'Dutch']\n",
        "\n",
        "    ng_related = {}\n",
        "    \n",
        "    x_train_grams = make_n_grams(x_train, y_train, n_gram_size, latin_languages)\n",
        "    sorted_tf_per_lang = sort_by_tf(x_train_grams, y_train)\n",
        "\n",
        "    for lang_key in latin_languages:\n",
        "        ng_related[lang_key] = []\n",
        "        latin_languages.remove(lang_key)\n",
        "        latin_langs = latin_languages\n",
        "        for otherlang in latin_langs:\n",
        "            top_m = list(sorted_tf_per_lang[lang_key].keys())[:m_samples]\n",
        "            top_m_x = list(sorted_tf_per_lang[otherlang].keys())[:m_samples]\n",
        "\n",
        "            # compares the two top m lists for common elements:\n",
        "            common_ngrams = list(set(top_m).intersection(top_m_x))\n",
        "                \n",
        "            if len(common_ngrams) > 3: # if two languages share 4 or more n-grams in their top n n-grams\n",
        "                \n",
        "                print(lang_key, \"and\", otherlang, \"have the following frequent\", n_gram_size,\"-grams in common:\",common_ngrams)\n",
        "                ng_related[lang_key].append(otherlang)\n",
        "                \n",
        "            # find counts of the entries in common_ngrams for each language.\n",
        "            # These are stored as the values corresponding to the ngram keys in the dictionary\n",
        "\n",
        "                counts_langkey = []\n",
        "                counts_otherlang = []\n",
        "                for i in common_ngrams:\n",
        "                    counts_langkey.append(sorted_tf_per_lang[lang_key][i])\n",
        "                    counts_otherlang.append(sorted_tf_per_lang[otherlang][i])\n",
        "\n",
        "                common_ngrams = [k.replace(' ', '_') for k in common_ngrams]\n",
        "\n",
        "                # code for bar chart:\n",
        "                x = np.arange(len(common_ngrams))  # the label locations\n",
        "                width = 0.35  # the width of the bars\n",
        "\n",
        "                fig, ax = plt.subplots()\n",
        "                rects1 = ax.bar(x - width/2, counts_langkey, width, color = 'r', label=lang_key)\n",
        "                rects2 = ax.bar(x + width/2, counts_otherlang, width, color = 'g', label=otherlang)\n",
        "                ax.set_ylabel('Count')\n",
        "                ax.set_title('Frequency of %s-grams in given languages' % (n_gram_size))\n",
        "                ax.set_xticks(x)\n",
        "                ax.set_xticklabels(common_ngrams, fontsize=12)\n",
        "                ax.legend()\n",
        "                fig.tight_layout()\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "                #counts_langkey = []\n",
        "                #counts_otherlang = []\n",
        "\n",
        "        print('\\n ')                 \n",
        "    \n",
        "    print('similar languages based on ', n_gram_size, '- grams:')\n",
        "    for key, val in ng_related.items():\n",
        "        print(key, ':', val)\n",
        "    print('\\n ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3vFh28hYXfA"
      },
      "source": [
        "### Naive Bayes Classifier\n",
        "To obtain a baseline for the language identification task we employ a simple Naive Bayes classifier. Our first step is to collect the top n-grams into feature matrices..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlUCSGN9Wg3l"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import time,math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07I5K2SxKXWF"
      },
      "source": [
        "def time_since(since):\r\n",
        "    s = time.time() - since\r\n",
        "    m = math.floor(s / 60)\r\n",
        "    s -= m * 60\r\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stiyBC09ePY0"
      },
      "source": [
        "# extracts lists of top n frequent n-grams from data\r\n",
        "def get_top_n_features(X_grams, Y, n_features):\r\n",
        "  '''\r\n",
        "  X : [[ngram_in_ex1],[ngram_in_ex2],....]\r\n",
        "  Y : ['lang1','lang1',...,'lang2',...]\r\n",
        "  n_features : number of ngram to pick from each language\r\n",
        "  '''\r\n",
        "  sorted_freq_per_lang = sort_by_tf(X_grams, Y)\r\n",
        "  features = []\r\n",
        "  for lang, grams_dict in sorted_freq_per_lang.items():\r\n",
        "      i = 0\r\n",
        "      for gram, count in grams_dict.items():\r\n",
        "          if i <= n_features:\r\n",
        "              features.append(gram)\r\n",
        "          else:\r\n",
        "              break\r\n",
        "          i += 1\r\n",
        "      \r\n",
        "  return list(set(features))\r\n",
        "\r\n",
        "  # features : ['and','he ','öä ',....] Top ngrams from corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUr77bPVeVQm"
      },
      "source": [
        "# convert data to feature matrix\r\n",
        "def create_feature_matrix(X, features):\r\n",
        "  '''\r\n",
        "  X : [[ngram_in_ex1],[ngram_in_ex2],....]\r\n",
        "  features : ['and','he ','öä ',....] Top ngrams from corpus\r\n",
        "\r\n",
        "  '''\r\n",
        "  mat = np.zeros((len(X),len(features)))\r\n",
        "  i = 0\r\n",
        "  for gram_list in X:\r\n",
        "      gram_count = []\r\n",
        "      for gram in features:          \r\n",
        "          if gram in gram_list:\r\n",
        "              gram_count.append(gram_list.count(gram)+1)\r\n",
        "          else:\r\n",
        "              gram_count.append(1)\r\n",
        "      mat[i] = gram_count\r\n",
        "      i+=1\r\n",
        "\r\n",
        "  return mat\r\n",
        "  # mat : array([[4,1,2,1,...],[1,1,1,2,3,1,...],...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaeJeeEvRtDd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "bc68dbad-f232-4d29-c886-94476c5c626e"
      },
      "source": [
        "n_instances = 100 # instance per language\r\n",
        "n_gram_size = 5\r\n",
        "n_features = 10 # features per language\r\n",
        "start = time.time()\r\n",
        "print(\"Starting preprocessing at {} ..\".format(time_since(start)))\r\n",
        "\r\n",
        "x_train, y_train = preprocess(X_train[:-1], Y_train[:-1])\r\n",
        "\r\n",
        "print(\"Preprocessing Done at {}.\".format(time_since(start)))\r\n",
        "\r\n",
        "# reduce languages to get smaller data subset\r\n",
        "x_train, y_train = get_data_chunk(x_train, y_train, n_instances)\r\n",
        "\r\n",
        "print(\"Making ngrams at {} ..\".format(time_since(start)))\r\n",
        "x_train_grams = make_n_grams(x_train, y_train, n_gram_size)\r\n",
        "\r\n",
        "print('Extracting features at {} ...'.format(time_since(start)))\r\n",
        "# create features for dataset\r\n",
        "features = get_top_n_features(x_train_grams,y_train,n_features) # Creating features from whole dataset ????\r\n",
        "\r\n",
        "print('Creating feature matrix at {}  ....'.format(time_since(start)))\r\n",
        "# Convert dataset into feature matrix\r\n",
        "feature_matrix = create_feature_matrix(x_train_grams, features)\r\n",
        "print('Data Preperation completed after {}'.format(time_since(start)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-b9126fcac634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_gram_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;31m# features per language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convert dataset into feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-00998bf458bc>\u001b[0m in \u001b[0;36mget_top_n_features\u001b[0;34m(X, Y, n_features, n_gram_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extracts lists of top n frequent n-grams from data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_top_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msorted_freq_per_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_by_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_n_grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: make_n_grams() missing 1 required positional argument: 'n'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsIdn1iIRu2g"
      },
      "source": [
        "# Gaussian Naive Bayes Model Training\r\n",
        "encoder = LabelEncoder()\r\n",
        "X = feature_matrix\r\n",
        "Y = encoder.fit_transform(y_train)\r\n",
        "model = GaussianNB()\r\n",
        "model.fit(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjTptZN9Rxxl"
      },
      "source": [
        "# model testing\r\n",
        "start = time.time()\r\n",
        "print('Test Data preperation starting ...')\r\n",
        "x,y = preprocess(X_test[:20000], Y_test[:20000])\r\n",
        "\r\n",
        "print(\"Making ngrams at {} ..\".format(time_since(start)))\r\n",
        "x_test_grams = make_n_grams(x, y, n_gram_size)\r\n",
        "\r\n",
        "print('Creating feature matrix at {}  ....'.format(time_since(start)))\r\n",
        "x = create_feature_matrix(x_test_grams, features)\r\n",
        "\r\n",
        "print('Test Data Preperation completed after {}'.format(time_since(start)))\r\n",
        "\r\n",
        "y = encoder.fit_transform(y)\r\n",
        "y_pred = model.predict(x)\r\n",
        "conf_matrix = confusion_matrix(y_pred=y_pred, y_true=y)\r\n",
        "acc = round(accuracy_score(y_pred=y_pred, y_true=y), 4) * 100\r\n",
        "print(f\"Accuracy is {acc}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppf3M1lWS9BS"
      },
      "source": [
        "# Model                Instance_per_language   N_gram       Features_per_language         Accuracy             Test_Instance\r\n",
        "# GaussianNB              150                     3                   40                     79%                   20k\r\n",
        "# GaussianNB              150                     4                   40                     87%                   20k\r\n",
        "# GaussianNB              150                     5                   40                     87%                   25k\r\n",
        "# GaussianNB              200                     5                   30                     85%                   25k\r\n",
        "# MultinomialNB           150                     3                   40                     77%                   25k  \r\n",
        "# MultinomialNB           150                     4                   40                     73%                   25k  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC1mI32qeqY8"
      },
      "source": [
        "### RNN Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3fZ8Vrmd5hi"
      },
      "source": [
        "import torch\r\n",
        "import time\r\n",
        "from torch.autograd import Variable \r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmHu-KycMevY"
      },
      "source": [
        "**CHANGE TO GPU MODE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFTaWPSMdqO"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPUswhM1qH8X"
      },
      "source": [
        "class RNN(nn.Module):\r\n",
        "    def __init__(self,input_size,hid_size,output_size,layers,embedding):\r\n",
        "\r\n",
        "        super(RNN, self).__init__()\r\n",
        "        self.hidden_dim = hid_size\r\n",
        "        self.layers = layers\r\n",
        "        self.embedding_size = embedding \r\n",
        "        self.dropout = nn.Dropout(0.4)\r\n",
        "        self.input_size = input_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.embeddings = nn.Embedding(self.input_size,self.embedding_size)\r\n",
        "        self.rnn = nn.GRU(input_size=self.embedding_size,hidden_size=self.hidden_dim,num_layers = self.layers)\r\n",
        "        self.linear = nn.Linear(self.hidden_dim,self.output_size)\r\n",
        "\r\n",
        "    \r\n",
        "    def forward(self,x):\r\n",
        "        # x : B x S where B is batch size and S is sequence size\r\n",
        "        # Sequence size is length of one ngram vector encoding\r\n",
        "        batch_size = x.size(0) \r\n",
        "        x = x.t() \r\n",
        "        embedded = self.embeddings(x) # S x B x I , here I is input_size/vocab size\r\n",
        "        hidden = self._init_hidden(batch_size)\r\n",
        "        output,hidden = self.rnn(embedded,hidden)\r\n",
        "        output = self.dropout(output) \r\n",
        "        fc_output = self.linear(output[-1]) # B x L , L is number of classes/languages\r\n",
        "        return fc_output\r\n",
        "\r\n",
        "\r\n",
        "    def _init_hidden(self,batch_size):\r\n",
        "        hidden_state = torch.zeros(self.layers,batch_size, self.hidden_dim, device=device)\r\n",
        "        return hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ennmQVVXqfI4"
      },
      "source": [
        "def padding(vector_inps, lengths ):\r\n",
        "  '''\r\n",
        "  This function takes variable lengths vectors and convert them into equal length by padding 0\r\n",
        "  Input : \"vector_inps\" list of vectors containing indices of ngrams\r\n",
        "          \"lengths \" length of each vector\r\n",
        "  Output : tensor containing vectors of equal length after padding. This length is equal to maximum number(M) in list of \"lengths\".\r\n",
        "  '''\r\n",
        "  inp_tensor = torch.zeros((len(vector_inps),lengths.max()), device= device).long()\r\n",
        "  for idx, (seq, seq_len) in enumerate(zip(vector_inps,lengths)):\r\n",
        "    inp_tensor[idx, :seq_len] = torch.LongTensor(seq)\r\n",
        "  return inp_tensor\r\n",
        "\r\n",
        "  # inp_tensor : tensor([[12,342,...,0],[56,2311,....],....])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU5wFzymqguQ"
      },
      "source": [
        "def train(decoder, inp, target, batch = 100):\r\n",
        "  '''\r\n",
        "  decoder : Model\r\n",
        "  inp  : tensor([[12,342,...,0],[56,2311,....],....]) Example encodings\r\n",
        "  target : tensor([41,127,234,16,....]) Label Encodings\r\n",
        "  '''\r\n",
        "    decoder.zero_grad()\r\n",
        "    loss = 0\r\n",
        "    for i in range(1,int(len(inp)/batch)):\r\n",
        "        output = decoder(inp[(i-1)*batch:(i)*batch].view(batch,-1)) # Input to model should be in form B x S where B is batch size and S is sequence size\r\n",
        "        loss += criterion(output,target[(i-1)*batch:(i)*batch])\r\n",
        "    loss.backward()\r\n",
        "    decoder_optimizer.step()\r\n",
        "\r\n",
        "    return loss.item()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRlIup7rK5RY"
      },
      "source": [
        "def create_Encodings(X_grams,Y,word_to_ix):\r\n",
        "  '''\r\n",
        "  X_grams : [[ngram_in_ex1],[ngram_in_ex2],....]\r\n",
        "  Y : ['lang1','lang1',...,'lang2',....]\r\n",
        "  word_to_ix : {' of':1,'apf':2,....} This is vocabulary of selected top ngrams\r\n",
        "  '''\r\n",
        "  x_grams = []\r\n",
        "  y_grams = []\r\n",
        "  gram_len = []\r\n",
        "  iter = 500\r\n",
        "  print('Creating Encoding for X')\r\n",
        "  for j in range(len(X_grams)):\r\n",
        "    gramlist = X_grams[j] # list of ngrams in example j\r\n",
        "    grams = [word_to_ix[w] for w in gramlist if w in list(word_to_ix.keys()) ] \r\n",
        "\r\n",
        "    if(len(grams) >= 1): # resulting grams list must not be empty\r\n",
        "      x_grams.append(grams) # Add encodings to x_grams\r\n",
        "      gram_len.append(len(grams)) # Keeping track of number of grams in each sentence, it will be used for padding later\r\n",
        "      y_grams.append(Y[j]) # Add corresponding language to y_grams\r\n",
        "\r\n",
        "    if( j % iter == 0):\r\n",
        "      print(\"Iteration {} completed at time {}\".format(j,time_since(start)))\r\n",
        "\r\n",
        "  gram_len = torch.LongTensor(gram_len)\r\n",
        "  inp = padding(x_grams,gram_len) # Created input vector with equal size list for all example\r\n",
        "\r\n",
        "  print('Creating Encoding for Y')\r\n",
        "  label = list(set(y_grams))\r\n",
        "  labels_to_idx = { lang:i  for i,lang in enumerate(label)}\r\n",
        "  y_label = torch.zeros(len(y_grams),device=device).long()\r\n",
        "  for i in range(len(y_label)):\r\n",
        "    y_label[i] = labels_to_idx[y_grams[i]] # Encoding each training label\r\n",
        "  target = Variable(torch.cuda.LongTensor(y_label))\r\n",
        "  return inp,target\r\n",
        "\r\n",
        "  # inp  : tensor([[12,342,...,0],[56,2311,....],....])\r\n",
        "  # target : tensor([41,127,234,16,....])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snUkxMh3vGjJ"
      },
      "source": [
        "# GPU MODE AHEAD\r\n",
        "n_instances = 130 # instance per language\r\n",
        "n_gram_size = 4\r\n",
        "n_features = 15 # features per language\r\n",
        "start = time.time()\r\n",
        "print(\"Starting preprocessing at {} ..\".format(time_since(start)))\r\n",
        "\r\n",
        "x_train, y_train = preprocess(X_train[:-1], Y_train[:-1])\r\n",
        "\r\n",
        "print(\"Preprocessing Done at {}.\".format(time_since(start)))\r\n",
        "\r\n",
        "# reduce languages to get smaller data subset\r\n",
        "x_train, y_train = get_data_chunk(x_train, y_train, n_instances)\r\n",
        "\r\n",
        "print(\"Making ngrams at {} ..\".format(time_since(start)))\r\n",
        "x_train_grams = make_n_grams(x_train, y_train, n_gram_size)\r\n",
        "\r\n",
        "print('Extracting features at {} ...'.format(time_since(start)))\r\n",
        "# create features for dataset\r\n",
        "features = get_top_n_features(x_train_grams,y_train,n_features) # Creating features from whole dataset ????\r\n",
        "word_to_ix = {word: i for i, word in enumerate(features)}\r\n",
        "\r\n",
        "print('Preparing encoding at {}'.format(time_since(start)))\r\n",
        "inp, target = create_Encodings(x_train_grams,y_train,word_to_ix)\r\n",
        "print('Encoding completed at {}'.format(time_since(start)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Vho_QiLCF8"
      },
      "source": [
        "print('Training Phase..')\r\n",
        "hidden_size = 64\r\n",
        "input_size = len(features) # Number of grams in vocabulary\r\n",
        "output_size = len(set(target)) # Number of languages\r\n",
        "n_layers = 1 # Number of layers of RNN\r\n",
        "embedding_size = 64\r\n",
        "lr = 0.001\r\n",
        "model = RNN(input_size,hidden_size,output_size,n_layers,embedding_size).to(device)\r\n",
        "decoder_optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "start = time.time()\r\n",
        "print_every = 500\r\n",
        "all_losses = []\r\n",
        "loss_avg = 0\r\n",
        "n_epochs = 3000\r\n",
        "inp = create_batch(x_train,3)\r\n",
        "for epoch in range(1, n_epochs+1):\r\n",
        "    loss = train(model,inp,target)\r\n",
        "    all_losses.append(loss)\r\n",
        "    if(epoch%print_every == 0):\r\n",
        "      print('[{} ({} {}%) {:.4f}]'.format(time_since(start), epoch, epoch/n_epochs * 100, loss))\r\n",
        "\r\n",
        "print(\"Training Complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz039G3cLEFB"
      },
      "source": [
        "def predict(model, test_x, test_y):\r\n",
        "  '''\r\n",
        "  decoder : Model\r\n",
        "  test_x  : tensor([[12,342,...,0],[56,2311,....],....]) Example encodings\r\n",
        "  true : tensor([41,127,234,16,....]) Label Encodings\r\n",
        "\r\n",
        "  '''\r\n",
        "  out = model(test_x) # out : B x L, where B is batch size and L is number of Labels/Classes\r\n",
        "  out = out.argmax(dim=1)\r\n",
        "  correct = out.eq(test_y.data.view_as(out)).cpu().sum()\r\n",
        "  print(correct)\r\n",
        "  accuracy = (correct/len(test_x))*100\r\n",
        "  print(\"Accuracy is {}%\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQkFVzOlLFrS"
      },
      "source": [
        "# model testing\r\n",
        "start = time.time()\r\n",
        "print('Test Data preperation starting ...')\r\n",
        "x,y = preprocess(X_test[:20000], Y_test[:20000])\r\n",
        "\r\n",
        "print(\"Making ngrams at {} ..\".format(time_since(start)))\r\n",
        "x_test_grams = make_n_grams(x, y, n_gram_size)\r\n",
        "\r\n",
        "print('Preparing encoding at {}'.format(time_since(start)))\r\n",
        "inp_t, true = create_Encodings(x_test_grams,y,word_to_ix)\r\n",
        "print('Encoding completed at {}'.format(time_since(start)))\r\n",
        "\r\n",
        "predict(model,inp_t,true)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}