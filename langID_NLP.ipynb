{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of langID_NLP.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dagobert42/langID-NLP/blob/main/langID_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt1fyVMmyyc5"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import string\r\n",
        "import re\r\n",
        "from nltk import ngrams\r\n",
        "from collections import defaultdict\r\n",
        "import nltk\r\n",
        "import collections\r\n",
        "import torch\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcHt-U5X0Qk0"
      },
      "source": [
        "# read data\r\n",
        "# this was written for the WiLI-2018 data set: https://zenodo.org/record/841984\r\n",
        "# make sure txt-files are in the specified directory when running this \r\n",
        "X_train = open('x_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_train = open('y_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "X_test = open('x_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_test = open('y_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "labels = pd.read_csv('labels.csv', delimiter = ';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgaeru6_AWpW"
      },
      "source": [
        "# Preprocessing the data\r\n",
        "\r\n",
        "def preprocess(X,Y):\r\n",
        "  # remove unnecessary characters from data\r\n",
        "  extras = '!\"$%&/{}[]()=?\\\\`´*+~#-_.:,;<>|1234567890°-\\'' # Characters to remove from data\r\n",
        "  rx = '[' + re.escape(''.join(extras)) + ']'\r\n",
        "  x_train = [] \r\n",
        "  for example in X:\r\n",
        "      x_train.append(re.sub(' +', ' ', re.sub(rx, '', example)))\r\n",
        "    \r\n",
        "  # convert language labels to language Name => 'en' -> 'English'\r\n",
        "  lab_dict = { labels.loc[i]['Label'] : labels.loc[i]['English'] for i in range(0, len(labels)) }\r\n",
        "  y_train = [ lab_dict[item] if item != 'nan' else 'Min Nan Chinese' for item in Y ]\r\n",
        "\r\n",
        "  return x_train,y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh1WIZ_bAkZA"
      },
      "source": [
        "# ordering sentences by language\r\n",
        "def data_by_lang(X,Y): \r\n",
        "  lang_corpora = defaultdict(list)\r\n",
        "  lang_idx = defaultdict(list)\r\n",
        "  for i in range(len(X)):\r\n",
        "      lang_corpora[Y[i]].append(X[i])\r\n",
        "      lang_idx[Y[i]].append(i)\r\n",
        "  return lang_corpora,lang_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNBMCn3bA078"
      },
      "source": [
        "# Get data with same proportion of exxample from each languages\r\n",
        "def data_chunk(X,Y,n_instances):\r\n",
        "  _,lang_idx = data_by_lang(X,Y)\r\n",
        "  x_train = []\r\n",
        "  y_train = []\r\n",
        "  for lang in list(set(Y)):\r\n",
        "    indices = lang_idx[lang]\r\n",
        "    for index in range(n_instances):\r\n",
        "      x_train.append(X[indices[index]])\r\n",
        "      y_train.append(Y[indices[index]])\r\n",
        "\r\n",
        "  return x_train,y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_gdP0c8ZGtJ"
      },
      "source": [
        "# creating n-grams for each language\r\n",
        "# data has to be a dict of lang : corpus\r\n",
        "# returns a dict of lang : n-grams\r\n",
        "def n_grams_per_lang(n, data):\r\n",
        "    gram_per_lang = defaultdict(list)\r\n",
        "    for lang in data.keys():\r\n",
        "        for sent in data[lang]:\r\n",
        "            gram_per_lang[lang] += [sent[i:i+n] for i in range(len(sent)-n+1)]\r\n",
        "    \r\n",
        "    return gram_per_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT5Bp8t6ZJyz"
      },
      "source": [
        "# counting and sorting n-grams for each language\r\n",
        "# data has to be a dict of lang : n-grams\r\n",
        "# returns a sorted dict of lang : {n-gram : count}\r\n",
        "def sort_by_tf(data):\r\n",
        "\r\n",
        "    # calculating term frequency of n-grams per language\r\n",
        "    tf_per_lang = defaultdict(list)\r\n",
        "    for lang in data.keys():\r\n",
        "        tf_per_lang[lang] = dict(zip(list(collections.Counter(data[lang]).keys()), list(collections.Counter(data[lang]).values())))\r\n",
        "\r\n",
        "    # sort by term frequency\r\n",
        "    sorted_tf_per_lang = defaultdict(list)\r\n",
        "    for lang in data.keys():\r\n",
        "        sorted_tf_per_lang[lang] = { word : value for word, value in sorted(tf_per_lang[lang].items(), key=lambda item:item[1], reverse=True) }\r\n",
        "    \r\n",
        "    return sorted_tf_per_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2m7X2FGbQwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d6f2e5-af92-420e-fef2-8cda403c27cf"
      },
      "source": [
        "# make some n-grams and print examples\n",
        "for n in range(3, 6):\n",
        "    ngpl = n_grams_per_lang(n, lang_corpora)\n",
        "    sorted_tf_per_lang = sort_by_tf(ngpl)\n",
        "\n",
        "    # print some examples\n",
        "    languages = ['German', 'English', 'Arabic']\n",
        "    n_samples = 10\n",
        "    for lang_key in languages:\n",
        "        print(lang_key, ':', n, '- grams')\n",
        "        print(list(sorted_tf_per_lang[lang_key].keys())[:n_samples])\n",
        "        print(list(sorted_tf_per_lang[lang_key].values())[:n_samples])\n",
        "    print('##########################')\n",
        "\n",
        "    latin_languages = ['German', 'English', 'French', 'Spanish', 'Italian', 'Portugese', 'Estonian',\n",
        "                        'Turkish', 'Romanian', 'Swedish', 'Latin', 'Dutch']\n",
        "    ng_related = {}\n",
        "    \n",
        "    for lang_key in latin_languages:\n",
        "        for otherlang in latin_languages:\n",
        "            top20 = list(sorted_tf_per_lang[lang_key].keys())[:n_samples]\n",
        "            if otherlang == lang_key:\n",
        "                continue\n",
        "            else:\n",
        "                top20x = list(sorted_tf_per_lang[otherlang].keys())[:n_samples]\n",
        "                # compares the two top 20 lists for common elements:\n",
        "                common_ngrams = list(set(top20).intersection(top20x))\n",
        "            \n",
        "                \n",
        "                if len(common_ngrams) > 0:\n",
        "                    ng_related[lang_key] = otherlang\n",
        "                \n",
        "\n",
        "    print('common '+n+ '- grams dictionary: ')\n",
        "    print(ng_related)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German : 3 - grams\n",
            "['en ', 'er ', ' de', 'der', 'sch', 'ie ', 'che', 'nd ', 'ein', 'ch ']\n",
            "[3915, 3021, 2241, 1655, 1473, 1336, 1184, 1175, 1101, 1073]\n",
            "English : 3 - grams\n",
            "[' th', 'he ', 'the', 'ed ', ' in', ' an', 'nd ', 'and', ' of', 'of ']\n",
            "[2838, 2765, 2569, 1660, 1371, 1326, 1287, 1277, 1240, 1182]\n",
            "Arabic : 3 - grams\n",
            "[' ال', 'الم', 'ية ', 'في ', ' في', 'ة ا', ' من', 'من ', 'ن ا', 'ات ']\n",
            "[8365, 1595, 1468, 1441, 1424, 1378, 1168, 1086, 1084, 937]\n",
            "##########################\n",
            "German : 4 - grams\n",
            "['der ', ' der', 'und ', ' die', ' und', 'den ', 'die ', 'ten ', 'sche', ' ein']\n",
            "[1316, 1137, 886, 863, 845, 800, 782, 687, 656, 631]\n",
            "English : 4 - grams\n",
            "[' the', 'the ', ' of ', 'and ', ' and', ' in ', 'ing ', ' to ', 'tion', 'ion ']\n",
            "[2368, 2149, 1179, 1149, 1067, 926, 900, 763, 630, 519]\n",
            "Arabic : 4 - grams\n",
            "[' الم', 'ة ال', ' في ', 'ن ال', ' من ', ' وال', 'ي ال', 'ت ال', ' الأ', ' الت']\n",
            "[1359, 1300, 1289, 1015, 948, 696, 686, 626, 611, 603]\n",
            "##########################\n",
            "German : 5 - grams\n",
            "[' der ', ' und ', ' die ', 'ische', 'chen ', ' von ', ' den ', ' eine', 'n der', 'schen']\n",
            "[1118, 844, 776, 522, 488, 470, 429, 394, 365, 346]\n",
            "English : 5 - grams\n",
            "[' the ', ' and ', 'n the', ' was ', 'tion ', ' of t', 'of th', 'ation', 'f the', ' for ']\n",
            "[2147, 1065, 449, 448, 408, 390, 368, 367, 359, 332]\n",
            "Arabic : 5 - grams\n",
            "[' على ', 'ات ال', 'في ال', ' في ا', 'من ال', ' من ا', 'ية ال', ' إلى ', 'ة في ', 'لى ال']\n",
            "[478, 456, 443, 421, 382, 366, 358, 323, 271, 264]\n",
            "##########################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeTA0UUDeHud"
      },
      "source": [
        "# Feature Extraction\r\n",
        "\r\n",
        "def get_features(data,n_features,n_gram):\r\n",
        "  sorted_freq_per_lang = sort_by_tf(n_grams_per_lang(n_gram,data))\r\n",
        "\r\n",
        "  features = []\r\n",
        "  for lang, grams_dict in sorted_freq_per_lang.items():\r\n",
        "      i = 0\r\n",
        "      for gram,count in grams_dict.items():\r\n",
        "          if i <=n_features:\r\n",
        "              features.append(gram)\r\n",
        "          else:\r\n",
        "              break\r\n",
        "          i+=1\r\n",
        "      \r\n",
        "  return list(set(features))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzRuZ1doRqqT"
      },
      "source": [
        "# Convert data into feature matrix\r\n",
        "\r\n",
        "def create_feature_matrix(data,n_instances,n_features):\r\n",
        "  mat = np.zeros((n_instances,n_features))\r\n",
        "  i = 0\r\n",
        "  for sent in data:\r\n",
        "      trigrams = [sent[i:i+3] for i in range(len(sent)-3+1)]\r\n",
        "      tri_dict = dict(zip(collections.Counter(trigrams).keys(),collections.Counter(trigrams).values()))\r\n",
        "      gram_count = []\r\n",
        "      for gram in features:\r\n",
        "          if gram in tri_dict.keys():\r\n",
        "              gram_count.append(tri_dict[gram]+1)\r\n",
        "          else:\r\n",
        "              gram_count.append(1)\r\n",
        "      mat[i] = gram_count\r\n",
        "      i+=1\r\n",
        "\r\n",
        "  return mat\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaeJeeEvRtDd"
      },
      "source": [
        "\r\n",
        "n_instances = 200 # instance per language\r\n",
        "x_train,y_train = preprocess(X_train[:-1],Y_train[:-1])\r\n",
        "x_train,y_train = data_chunk(x_train,y_train,n_instances)\r\n",
        "\r\n",
        "# Create features for dataset\r\n",
        "n_gram = 5\r\n",
        "n_features = 30 # features per language\r\n",
        "data,_ = data_by_lang(x_train,y_train)\r\n",
        "features = get_features(data,n_features,n_gram)\r\n",
        "\r\n",
        "# Convert dataset into feature matrix\r\n",
        "n_instances = len(x_train) # total instances in dataset\r\n",
        "n_features = len(features) # total features\r\n",
        "X = create_feature_matrix(x_train,n_instances,n_features,n_gram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsIdn1iIRu2g"
      },
      "source": [
        "# Gaussian Naive Bayes Model Training\r\n",
        "\r\n",
        "encoder = LabelEncoder()\r\n",
        "Y = encoder.fit_transform(y_train)\r\n",
        "model = GaussianNB()\r\n",
        "model.fit(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjTptZN9Rxxl"
      },
      "source": [
        "# Model Testing\r\n",
        "x,y = preprocess(X_test[:20000],Y_test[:20000])\r\n",
        "x = create_feature_matrix(x,len(x),n_features)\r\n",
        "y = encoder.fit_transform(y)\r\n",
        "y_pred = model.predict(x)\r\n",
        "conf_matrix = confusion_matrix(y_pred=y_pred,y_true=y)\r\n",
        "acc = round(accuracy_score(y_pred=y_pred,y_true=y),2) * 100\r\n",
        "print(f\"Accuracy is {acc}%\") \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppf3M1lWS9BS"
      },
      "source": [
        "# Model                Instance_per_language   N_gram       Features_per_language         Accuracy             Test_Instance\r\n",
        "# GaussianNB              150                     3                   40                     79%                   20k\r\n",
        "# GaussianNB              150                     4                   40                     87%                   20k\r\n",
        "# GaussianNB              150                     5                   40                     87%                   25k\r\n",
        "# GaussianNB              200                     5                   30                     85%                   25k\r\n",
        "# MultinomialNB           150                     3                   40                     77%                   25k  \r\n",
        "# MultinomialNB           150                     4                   40                     73%                   25k  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}