{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of langID_NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgCjPUC4Sk86"
      },
      "source": [
        "### Dataset Preparation\n",
        "This subsection contains methods to produce uniformly distributed chunks of our data set. From these we can then obtain n-grams of different sizes. The Wikipedia Language Identification database contains txt-files of x_train and x_test for example sentences and accordingly ordered labels in y_train, y_test.\n",
        "We read these examples and cluster them by their respective language label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt1fyVMmyyc5"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import string\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from nltk import ngrams\r\n",
        "import collections\r\n",
        "from collections import defaultdict\r\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v0_QzWx9XRU"
      },
      "source": [
        "# read data\r\n",
        "# written for the WiLI-2018 data set: https://zenodo.org/record/841984\r\n",
        "# make sure txt-files are in the specified directory when running this\r\n",
        "X_train = open('x_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_train = open('y_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "X_test = open('x_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_test = open('y_test.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "labels = pd.read_csv('labels.csv', delimiter = ';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njIRm3SO9xiV"
      },
      "source": [
        "# preprocessing the data\r\n",
        "def preprocess(X, Y):\r\n",
        "    # remove unnecessary characters\r\n",
        "    extras = '!\"$%&/{}[]()=?\\\\`´*+~#-_.:,;<>|1234567890°-\\''\r\n",
        "    rx = '[' + re.escape(''.join(extras)) + ']'\r\n",
        "    x_train = [] \r\n",
        "    for example in X:\r\n",
        "        x_train.append(re.sub(' +', ' ', re.sub(rx, '', example)))\r\n",
        "    \r\n",
        "    # convert language labels to language Name => 'en' -> 'English'\r\n",
        "    lab_dict = { labels.loc[i]['Label'] : labels.loc[i]['English'] for i in range(0, len(labels)) }\r\n",
        "    y_train = [ lab_dict[item] if item != 'nan' else 'Min Nan Chinese' for item in Y ]\r\n",
        "\r\n",
        "    return x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh1WIZ_bAkZA"
      },
      "source": [
        "# sort data by language\r\n",
        "def data_by_lang(X, Y):\r\n",
        "    lang_corpora = defaultdict(list)\r\n",
        "    lang_idx = defaultdict(list)\r\n",
        "    for i in range(len(X)):\r\n",
        "        lang_corpora[Y[i]] += X[i]\r\n",
        "        lang_idx[Y[i]].append(i)\r\n",
        "\r\n",
        "    return lang_corpora, lang_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNBMCn3bA078"
      },
      "source": [
        "# extract uniformly distributed list of examples from our data set\r\n",
        "# takes an optional argument to constrain the list of languages\r\n",
        "def get_data_chunk(X, Y, n_instances, lang_keys=[]):\r\n",
        "    _, lang_idx = data_by_lang(X, Y)\r\n",
        "    x_train = []\r\n",
        "    y_train = []\r\n",
        "    \r\n",
        "    langs = set()\r\n",
        "    if lang_keys: \r\n",
        "        langs = set(lang_keys)\r\n",
        "    else:\r\n",
        "        langs = set(Y)\r\n",
        "\r\n",
        "    for lang in langs:\r\n",
        "        indices = lang_idx[lang]\r\n",
        "        for index in range(n_instances):\r\n",
        "            x_train.append(X[indices[index]])\r\n",
        "            y_train.append(Y[indices[index]])\r\n",
        "\r\n",
        "    return x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_gdP0c8ZGtJ"
      },
      "source": [
        "# creating n-grams for each data entry\r\n",
        "# optional arguments:\r\n",
        "#    lang_keys - constrains the languages to use\r\n",
        "#    stepsize  - specifies the amount of characters\r\n",
        "#                to jump until the next n-gram\r\n",
        "# returns a list of n-grams\r\n",
        "def make_n_grams(X, Y, n, lang_keys=[], stepsize=1):\r\n",
        "    assert stepsize >= 1\r\n",
        "    x_to_grams = []\r\n",
        "\r\n",
        "    langs = set()\r\n",
        "    if lang_keys: \r\n",
        "        langs = set(lang_keys)\r\n",
        "    else:\r\n",
        "        langs = set(Y)\r\n",
        "\r\n",
        "    for i in range(len(X)):\r\n",
        "        if Y[i] in langs:\r\n",
        "            sent = X[i]\r\n",
        "            x_to_grams.append(list(sent[j:j+n] for j in range(0, len(sent) - n+1, stepsize)))\r\n",
        "\r\n",
        "    return x_to_grams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT5Bp8t6ZJyz"
      },
      "source": [
        "# counting and sorting n-grams for each language\r\n",
        "# returns a sorted dict of lang : {n-gram : count}\r\n",
        "def sort_by_tf(X, Y):\r\n",
        "    # calculating term frequency of n-grams per language\r\n",
        "    tf_per_lang = defaultdict(list)\r\n",
        "    langs = set(Y)\r\n",
        "    data, _ = data_by_lang(X, Y)\r\n",
        "    for lang in langs:\r\n",
        "        tf_per_lang[lang] = dict(\r\n",
        "            zip(list(Counter(data[lang]).keys()),\r\n",
        "                 list(Counter(data[lang]).values())))\r\n",
        "\r\n",
        "    # sort by term frequency\r\n",
        "    sorted_tf_per_lang = defaultdict(list)\r\n",
        "    for lang in langs:\r\n",
        "        sorted_tf_per_lang[lang] = { word : value for word, value in sorted(tf_per_lang[lang].items(), key=lambda item:item[1], reverse=True) }\r\n",
        "    \r\n",
        "    return sorted_tf_per_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3cYY3GVpMy"
      },
      "source": [
        "### Understanding Data\n",
        "In the following we review some examples to get an understanding of our data...\n",
        "Particularly interesting are languages with a degree of similarity. Here we print examples of languages that use the latin alphabet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUM_YDQ4dqkD"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC_uPl7fMbOf"
      },
      "source": [
        "x_train, y_train = preprocess(X_train[:-1],Y_train[:-1])\n",
        "\n",
        "lang_corpora, lang_idx = data_by_lang(x_train, y_train)\n",
        "# produce charts of counts of common n-grams in different languages, and a table suggesting similar languages based on these\n",
        "\n",
        "for n_gram_size in range(3, 6):\n",
        "    m_samples = 20\n",
        "    latin_languages = ['German', 'English', 'French', 'Spanish', 'Italian', 'Portuguese', \n",
        "                       'Estonian', 'Turkish', 'Romanian', 'Swedish', 'Latin', 'Dutch']\n",
        "\n",
        "    ng_related = {}\n",
        "    \n",
        "    x_train_grams = make_n_grams(x_train, y_train, n_gram_size, latin_languages)\n",
        "    sorted_tf_per_lang = sort_by_tf(x_train_grams, y_train)\n",
        "\n",
        "    for lang_key in latin_languages:\n",
        "        ng_related[lang_key] = []\n",
        "        latin_languages.remove(lang_key)\n",
        "        latin_langs = latin_languages\n",
        "        for otherlang in latin_langs:\n",
        "            top_m = list(sorted_tf_per_lang[lang_key].keys())[:m_samples]\n",
        "            top_m_x = list(sorted_tf_per_lang[otherlang].keys())[:m_samples]\n",
        "\n",
        "            # compares the two top m lists for common elements:\n",
        "            common_ngrams = list(set(top_m).intersection(top_m_x))\n",
        "                \n",
        "            if len(common_ngrams) > 3: # if two languages share 4 or more n-grams in their top n n-grams\n",
        "                \n",
        "                print(lang_key, \"and\", otherlang, \"have the following frequent\", n_gram_size,\"-grams in common:\",common_ngrams)\n",
        "                ng_related[lang_key].append(otherlang)\n",
        "                \n",
        "            # find counts of the entries in common_ngrams for each language.\n",
        "            # These are stored as the values corresponding to the ngram keys in the dictionary\n",
        "\n",
        "                counts_langkey = []\n",
        "                counts_otherlang = []\n",
        "                for i in common_ngrams:\n",
        "                    counts_langkey.append(sorted_tf_per_lang[lang_key][i])\n",
        "                    counts_otherlang.append(sorted_tf_per_lang[otherlang][i])\n",
        "\n",
        "                common_ngrams = [k.replace(' ', '_') for k in common_ngrams]\n",
        "\n",
        "                # code for bar chart:\n",
        "                x = np.arange(len(common_ngrams))  # the label locations\n",
        "                width = 0.35  # the width of the bars\n",
        "\n",
        "                fig, ax = plt.subplots()\n",
        "                rects1 = ax.bar(x - width/2, counts_langkey, width, color = 'r', label=lang_key)\n",
        "                rects2 = ax.bar(x + width/2, counts_otherlang, width, color = 'g', label=otherlang)\n",
        "                ax.set_ylabel('Count')\n",
        "                ax.set_title('Frequency of %s-grams in given languages' % (n_gram_size))\n",
        "                ax.set_xticks(x)\n",
        "                ax.set_xticklabels(common_ngrams, fontsize=12)\n",
        "                ax.legend()\n",
        "                fig.tight_layout()\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "                #counts_langkey = []\n",
        "                #counts_otherlang = []\n",
        "\n",
        "        print('\\n ')                 \n",
        "    \n",
        "    print('similar languages based on ', n_gram_size, '- grams:')\n",
        "    for key, val in ng_related.items():\n",
        "        print(key, ':', val)\n",
        "    print('\\n ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3vFh28hYXfA"
      },
      "source": [
        "### Naive Bayes Classifier\n",
        "To obtain a baseline for the language identification task we employ a simple Naive Bayes classifier. Our first step is to collect the top n-grams into feature matrices..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlUCSGN9Wg3l"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stiyBC09ePY0"
      },
      "source": [
        "# extracts lists of top n frequent n-grams from data\r\n",
        "def get_top_n_features(X_grams, Y, n_features):\r\n",
        "  sorted_freq_per_lang = sort_by_tf(X_grams, Y)\r\n",
        "\r\n",
        "  features = []\r\n",
        "  for lang, grams_dict in sorted_freq_per_lang.items():\r\n",
        "      i = 0\r\n",
        "      for gram, count in grams_dict.items():\r\n",
        "          if i <= n_features:\r\n",
        "              features.append(gram)\r\n",
        "          else:\r\n",
        "              break\r\n",
        "          i += 1\r\n",
        "      \r\n",
        "  return list(set(features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUr77bPVeVQm"
      },
      "source": [
        "# convert data to feature matrix\r\n",
        "def create_feature_matrix(X, Y, n_instances, n_features, n_gram_size):\r\n",
        "#  mat = np.zeros((n_instances,n_features))\r\n",
        "#  i = 0\r\n",
        "#  for sent in data:\r\n",
        "#      trigrams = [sent[i:i+] for i in range(len(sent)-3+1)]\r\n",
        "#      tri_dict = dict(zip(collections.Counter(trigrams).keys(), collections.Counter(trigrams).values()))\r\n",
        "#      gram_count = []\r\n",
        "#      for gram in features:\r\n",
        "#          if gram in tri_dict.keys():\r\n",
        "#              gram_count.append(tri_dict[gram]+1)\r\n",
        "#          else:\r\n",
        "#              gram_count.append(1)\r\n",
        "#      mat[i] = gram_count\r\n",
        "#      i+=1\r\n",
        "\r\n",
        "  return mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaeJeeEvRtDd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "bc68dbad-f232-4d29-c886-94476c5c626e"
      },
      "source": [
        "n_instances = 200 # instance per language\r\n",
        "x_train, y_train = preprocess(X_train[:-1], Y_train[:-1])\r\n",
        "# reduce languages to get smaller data subset\r\n",
        "x_train, y_train = get_data_chunk(x_train, y_train, n_instances)\r\n",
        "\r\n",
        "# x_train_grams = make_n_grams(x_train, y_train, n_gram_size)\r\n",
        "\r\n",
        "# create features for dataset\r\n",
        "n_gram_size = 5\r\n",
        "n_features = 30 # features per language\r\n",
        "features = get_top_n_features(x_train_grams, y_train, n_features, n_gram_size)\r\n",
        "\r\n",
        "# Convert dataset into feature matrix\r\n",
        "n_instances = len(x_train) # total instances in dataset\r\n",
        "n_features = len(features) # total features\r\n",
        "feature_matrix = create_feature_matrix(x_train_grams, features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-b9126fcac634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_gram_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;31m# features per language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convert dataset into feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-00998bf458bc>\u001b[0m in \u001b[0;36mget_top_n_features\u001b[0;34m(X, Y, n_features, n_gram_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extracts lists of top n frequent n-grams from data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_top_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msorted_freq_per_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_by_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_n_grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: make_n_grams() missing 1 required positional argument: 'n'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsIdn1iIRu2g"
      },
      "source": [
        "# Gaussian Naive Bayes Model Training\r\n",
        "encoder = LabelEncoder()\r\n",
        "Y = encoder.fit_transform(y_train)\r\n",
        "model = GaussianNB()\r\n",
        "model.fit(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjTptZN9Rxxl"
      },
      "source": [
        "# model testing\r\n",
        "x,y = preprocess(X_test[:20000], Y_test[:20000])\r\n",
        "x = create_feature_matrix(x, len(x), n_features)\r\n",
        "y = encoder.fit_transform(y)\r\n",
        "y_pred = model.predict(x)\r\n",
        "conf_matrix = confusion_matrix(y_pred=y_pred, y_true=y)\r\n",
        "acc = round(accuracy_score(y_pred=y_pred, y_true=y), 4) * 100\r\n",
        "print(f\"Accuracy is {acc}%\")\r\n",
        "\r\n",
        "# confusion matrix plot\r\n",
        "plt.subplots(figsize=(10, 10))\r\n",
        "sns.heatmap(conf_matrix, annot=True,fmt=\".1f\", linewidths=1.5)\r\n",
        "plt.xlabel(\"Predicted Language\")\r\n",
        "plt.ylabel(\"Correct Language\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppf3M1lWS9BS"
      },
      "source": [
        "# Model                Instance_per_language   N_gram       Features_per_language         Accuracy             Test_Instance\r\n",
        "# GaussianNB              150                     3                   40                     79%                   20k\r\n",
        "# GaussianNB              150                     4                   40                     87%                   20k\r\n",
        "# GaussianNB              150                     5                   40                     87%                   25k\r\n",
        "# GaussianNB              200                     5                   30                     85%                   25k\r\n",
        "# MultinomialNB           150                     3                   40                     77%                   25k  \r\n",
        "# MultinomialNB           150                     4                   40                     73%                   25k  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC1mI32qeqY8"
      },
      "source": [
        "### RNN Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3fZ8Vrmd5hi"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPUswhM1qH8X"
      },
      "source": [
        "class RNN(nn.Module):\r\n",
        "    def __init__(self,input_size,hid_size,output_size,layers,embedding):\r\n",
        "\r\n",
        "        super(RNN, self).__init__()\r\n",
        "        self.hidden_dim = hid_size\r\n",
        "        self.layers = layers\r\n",
        "        self.embedding_size = embedding \r\n",
        "        self.input_size = input_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.embeddings = nn.Embedding(self.input_size,self.embedding_size)\r\n",
        "        self.rnn = nn.RNN(input_size=self.embedding_size,hidden_size=self.hidden_dim,num_layers = self.layers)\r\n",
        "        self.linear = nn.Linear(self.hidden_dim,self.output_size)\r\n",
        "\r\n",
        "    \r\n",
        "    def forward(self,x):\r\n",
        "        batch_size = x.size(0)\r\n",
        "\r\n",
        "        x = x.t()\r\n",
        "\r\n",
        "        #print(\" input \",x.size())\r\n",
        "        embedded = self.embeddings(x)\r\n",
        "        #print(\" embeddings \", embedded.size())\r\n",
        "\r\n",
        "        hidden = self._init_hidden(batch_size)\r\n",
        "        output,hidden = self.rnn(embedded,hidden)\r\n",
        "        #print(\" LSTM hidden output\", hidden.size())\r\n",
        "\r\n",
        "        fc_output = self.linear(hidden)\r\n",
        "        #print(\" fc output \", fc_output.size())\r\n",
        "        return fc_output\r\n",
        "\r\n",
        "\r\n",
        "    def _init_hidden(self,batch_size):\r\n",
        "        hidden_state = torch.zeros(self.layers,batch_size, self.hidden_dim, device=device)\r\n",
        "        return Variable(hidden_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlxrjMwmqdOV"
      },
      "source": [
        "def time_since(since):\r\n",
        "    s = time.time() - since\r\n",
        "    m = math.floor(s / 60)\r\n",
        "    s -= m * 60\r\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ennmQVVXqfI4"
      },
      "source": [
        "# Data Preperation\r\n",
        "# A list containing examples from every language is given as input\r\n",
        "# Each of these examples are broken down into ngrams of specific ngram say trigram\r\n",
        "# For each example, a vector is created which contains indices of trigrams which are also part of vocabulary (or feature set). Rest all the trigram of the example is discarded\r\n",
        "# These vectors are then padded with zero to make their size equal.\r\n",
        "def create_batch(data,ngram):\r\n",
        "  '''\r\n",
        "  This function prepares data as input to RNN .\r\n",
        "  Input : \"data\" is list of examples in different languages ; Size : N\r\n",
        "          \"ngram\"\r\n",
        "  Output : It returns a dataset in numerical form where numerical values are indices of grams in vocabulary ; Size N x M\r\n",
        "            Refer to padding() function to know about M\r\n",
        "  '''\r\n",
        "  tup = [ create_iv(example,ngram) for example in data]\r\n",
        "  inps = [ t[0] for t in tup]\r\n",
        "  lengths = torch.LongTensor([t[1] for t in tup])\r\n",
        "  return padding(inps,lengths)\r\n",
        "\r\n",
        "\r\n",
        "def create_iv(data,n):\r\n",
        "  '''\r\n",
        "  This function is used to convert a text into list of numerical based on bag of words\r\n",
        "  Input : \"data\" is a sentence/paragraph in a language.\r\n",
        "          \"n\" is ngram size\r\n",
        "  Output : tuple(A,B) where A is list of ngrams indices wrt vocabulary and B is length of list A\r\n",
        "\r\n",
        "  '''\r\n",
        "  ngrams = [data[i:i+n] for i in range(len(data)-n+1)]\r\n",
        "  gram_idx =[word_to_ix[w] for w in ngrams if w in list(word_to_ix.keys()) ]\r\n",
        "  return gram_idx, len(gram_idx)\r\n",
        "    \r\n",
        "\r\n",
        "def padding(vector_inps, lengths ):\r\n",
        "  '''\r\n",
        "  This function takes variable lengths vectors and convert them into equal length by padding 0\r\n",
        "  Input : \"vector_inps\" list of vectors containing indices of ngrams\r\n",
        "          \"lengths \" length of each vector\r\n",
        "  Output : tensor containing vectors of equal length after padding. This length is equal to maximum number(M) in list of \"lengths\".\r\n",
        "  '''\r\n",
        "  inp_tensor = torch.zeros((len(vector_inps),lengths.max()), device= device).long()\r\n",
        "  for idx, (seq, seq_len) in enumerate(zip(vector_inps,lengths)):\r\n",
        "    inp_tensor[idx, :seq_len] = torch.LongTensor(seq)\r\n",
        "  return inp_tensor\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU5wFzymqguQ"
      },
      "source": [
        "def train(decoder, inp, target):\r\n",
        "    decoder.zero_grad()\r\n",
        "    loss = 0\r\n",
        "    output = decoder(inp)\r\n",
        "    #target = target.type_as(output)\r\n",
        "    loss += criterion(output[0], target)\r\n",
        "    loss.backward()\r\n",
        "    decoder_optimizer.step()\r\n",
        "\r\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snUkxMh3vGjJ"
      },
      "source": [
        "n_instances = 150 # instance per language\r\n",
        "x_train,y_train = preprocess(X_train[:-1],Y_train[:-1])\r\n",
        "x_train,y_train = get_data_chunk(x_train,y_train,n_instances)\r\n",
        "\r\n",
        "label = list(set(y_train)) # List of all languages\r\n",
        "labels_to_idx = { lang:i  for i,lang in enumerate(label)} # Dictionary of languages with a integer assigned to all, {'English':0,'Arabic':1,....}\r\n",
        "y_label = torch.zeros(len(y_train),device=device).long()\r\n",
        "for i in range(len(y_label)):\r\n",
        "  y_label[i] = labels_to_idx[y_train[i]] \r\n",
        "\r\n",
        "\r\n",
        "# Creating vocabulary using top ngrams from each language\r\n",
        "n_gram = 3\r\n",
        "top_n = 50 # top ngrams per language\r\n",
        "data,_ = data_by_lang(x_train,y_train)\r\n",
        "bow = get_bow(data,top_n,n_gram) # This will act as vocabulary\r\n",
        "word_to_ix = {word: i for i, word in enumerate(bow)} \r\n",
        "\r\n",
        "hidden_size = 64\r\n",
        "input_size = len(bow) # Number of grams in vocabulary\r\n",
        "output_size = len(label) # Number of languages\r\n",
        "n_layers = 1 # Number of layers of RNN\r\n",
        "embedding_size = 64\r\n",
        "lr = 0.001\r\n",
        "model = RNN(input_size,hidden_size,output_size,n_layers,embedding_size).to(device)\r\n",
        "decoder_optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "start = time.time()\r\n",
        "print_every = 10\r\n",
        "all_losses = []\r\n",
        "loss_avg = 0\r\n",
        "n_epochs = 30\r\n",
        "inp = create_batch(x_train,3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}